# 贝叶斯分类
指基于贝叶斯公式的分类问题
## 贝叶斯定理（前置）
详见《张宇概率论9讲》
* 公式
    
    首先给出概率论中的贝叶斯公式：
    $$P(A|B)=\frac{P(B|A)P(A)}{P(B)} $$
    公式展示了后验概率和先验概率之间的关系
    
    其中$P(B) $还可以进一步用全概率公式拆分
* 说人话

    这是一种从结果推断原因的思维，接下来将公式具象化为：
    $$P(张三偷的|东西被偷)=\frac{P(东西被偷|张三去偷)P(张三去偷)}{P(东西被偷)} $$
    若小偷不止张三一个，那么分母还可以用全概率公式拆分：
    $$P(张三偷的|东西被偷)=\frac{P(东西被偷|张三去偷)P(张三去偷)}{P(东西被偷)} 
    \\= \frac{P(东西被偷|张三去偷)P(张三去偷)}{P(东西被偷|张三)P(张三去偷)+P(东西被偷|李四)P(李四去偷)+P(东西被偷|王五)P(王五去偷)} 
    $$    
## 朴素贝叶斯分类
* 公式

    要计算在给定特征的情况下，结果为是或否的可能性：
    $$\omega_{MAP}=\arg{\max{P(\omega_i|a_1,a_2,\cdots,a_n)}} $$
    用上述的贝叶斯公式展开得到：
    $$\omega_{MAP}=\arg{\max{\frac{P(a_1,a_2,\cdots,a_n|\omega_i)P(\omega_i)}{P(a_1,a_2,\cdots,a_n)}}} $$
    注意到，分母为由数据集特征构成的联合分布概率，是常量，故实际要最大化的概率为：
    $$\omega_{MAP}=
     \arg{\max{P(a_1,a_2,\cdots,a_n|\omega_i)P(\omega_i)}} $$
    由于多维分布概率不太可能计算，实际应用中假定条件概率互相独立（条件概率独立不等于概率独立），即：
    $$P(a_j|\omega_i)与P(a_k|\omega_i)独立\\
    P(a_j|\omega_i,a_k)=P(a_j|\omega_i)$$
    举一个简单的例子：
    $$P(a_1,a_2|\omega_i)=P(a_1|\omega_i)P(a_2|\omega_i) $$
    因此，真正要求的内容如下：
    $$\omega_{MAP}=\arg{\max{P(\omega_i)}\prod_j{P(\alpha_j|\omega_i)}} $$
    $$w_i表示w属于第i类，P(w_i)=\frac{w属于第i类的示例数}{w所有的示例}$$
    $$\alpha_j表示命题，要具体情况具体分析 $$
    argmax表示，对w属于第i类的所有计算过后，最大值所在的i表示w最有可能属于第i类
* 条件概率独立

    条件概率相互独立与古典概型概率独立并不一样，详见《概率论9讲》联合概率分布中的条件概率
* laplace smoothing拉普拉斯平滑

    这是一种阻止朴素贝叶斯分类公式中的连乘项为0的防备手段。
    
    使用条件概率公式和古典概型计算$P(a_j|\omega_i)$，令：
    $$A = 打游戏多于10天的学生数 \\B = 挂科的学生数\\ N=学生总数$$
    $$P(打游戏10多天|挂科)=\frac{挂科且打游戏\geq10天的示例数}{结果为“挂科”的示例数}$$
    $$P(A|B)=\frac{A\bigcap B}{B}$$
    但有时整个系的学生都勤劳用功，这将导致(1)中的分子为0

    为了阻止连乘项为0，在分子分母都加上一个数

    $$P(打游戏10多天|挂科)=\frac{挂科且打游戏\geq10天的示例数+1}{结果为“挂科”的示例数+总示例数}$$
    $$P(A|B)=\frac{A\bigcap B+1}{B+N}$$
# 决策树分类

* 说人话

    用于分类，而且是从根结点到叶结点一步一步细分训练集

    细分的目的是 ___获取能快速分类的特征字段___

    就像劣迹艺人对粉丝逐步提纯，也能获得死忠粉
    
    比如推销人员判断潜在客户的话术:
    $$你是否有装修需求\begin{cases} 有！\longrightarrow一顿推销\longrightarrow\begin{cases}表示感兴趣\longrightarrow潜在客户\\
    未表态\longrightarrow路人 \end{cases}\\
    没有！\longrightarrow\begin{cases}
    有耐心听介绍\longrightarrow潜在客户\\
    没耐心听介绍\longrightarrow路人
    
    \end{cases}
    \end{cases} $$

    数据集可以产生多种决策树，但是决策树的结构还是简单、平衡一些好。
## Iterative Dichotomizer 3
这是一种决策数的构建算法，这里将记录一些前置基础
* 说人话

        好学生最大的特征是努力第一聪明第二，如果用某个度量努力程度的方法来作为决策树中划分学生的第一层，效果将非常显著。
    因此**特征与特征之间是存在优先级**，也可以理解为**矛盾的主要方面和次要方面**
    
    而决策树的**首要**工作就是找主要矛盾
* 特征划分

    1. 对于离散值，某一特征将划分为离散值的有限个类别
    2. 对于连续值，某一特征将按（一个或多个）阈值划分为有限个区间
* 信息熵

    是一种用于衡量信息不确定度的度量值
    $$Entropy(S)=-\sum_{i=1}^{n}p_i\log_2{p_i} $$
    $$在二分类中S\rightarrow[结果为1的概率，结果为0的概率]$$
* 信息增益

    是一种基于信息熵的衡量矛盾主要程度的度量值
    
    根据某一个特征A对数据集进行切分，再分别计算切分后数据集的信息熵，相减之后的值为该特征的信息增益

    
    比如计算“A=英语成绩等第”在“S=评上了奖学金”中的信息增益，将等第分为"A"、"B"、"C"：
    $$Gain(S,A)=Entropy(S)-\sum_{a\in A}\frac{|S_a|}{|S|}Entropy(S_a)\\
    =Entropy(S)-(\frac{等第为A的学生数}{学生总数}Entropy(S_{"A"})+\frac{等第为B的学生数}{学生总数}Entropy(S_{"B"})+\frac{等第为C的学生数}{学生总数}Entropy(S_{"C"})) $$
* 基尼杂质系数
  
    表示在样本集合中，一个随机选中的样本被分错的概率。
    $$G=1-\sum_{k=1}^{K}p_k^2 $$
    $$p_k\rightarrow一个样本被归入第k类的概率 $$
    $$划分前基尼\rightarrow古典概型 \frac{某一类的样本数}{总数}$$
* 基尼增益系数
  
    $$ 划分前基尼-(某个分支中\rightarrow左分支样本数*左基尼 + 右分支样本数*右基尼) $$
* 熵偏移entropy bias
  
    因为算法会试图在具体化的数据中找矛盾的主要方面，那么生日学号这些无关紧要的方面往往会具有较大的信息增益(从而得出"出生在4月的人成绩都很好")

    为了减少这些无谓特征的重要性，引入了splitInformation项：
    $$splitInformation(S,A)=-\sum_{a\in A}\frac{|S_a|}{|S|}\log_2\frac{|S_a|}{|S|} $$
    再引入惩罚项：
    $$GainRatio(S,A)=\frac{Gain(S,A)}{splitInformation(S,A)} $$
    信息增益越大，惩罚越大，从而大幅度降低决策树选择这些特征进行划分数据集的可能性
* ID3算法步骤

    这个算法步骤并不是太重点，这个算法的实现并不在本课程中给出

    1. 创建根结点
    2. 选择一个特征对数据集进行分类（如果还存在可选特征）
        
        1. 若分类后的某一边的数量为0，则认为是分类完毕
        2. 如果两边数量均不为0，在剩余的特征集中继续作分类

    要依靠**剪枝**来简化在训练集中生成的决策树以防止过拟合
