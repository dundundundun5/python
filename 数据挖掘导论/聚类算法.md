# clustering聚类
* 特点
  
    1. 簇内成员互相很靠近
    2. 簇间成员有明显区别
* 要求与注意

    1.  数值缩放直接影响聚类结果
    2.  分布改变也会影响聚类结果
    3.  需要定义对象之间的距离
    4.  需要能处理任意形状的数据
    5.  需要能处理噪点和异常点
## K-Means
* 过程描述
* 好

    1. 算法简单，簇都是球形形状
    2. 收敛快
    3. 反复计算距离，时间复杂度O(tkn)
* 坏

    1. K值，初始点很难选
    2. 容易收敛到局部最优
    3. 对噪点和异常点很敏感
    4. 不适合非凸数据
## Sequential Leader Clustering
* 过程描述
* 好

    1. 没有迭代
    2. 只过一遍数据
* 坏

    1. 难以选择阈值
## Model Based Clustering(Gaussian Mixture)
高斯混合是用多个高斯模型聚类的算法
* 公式描述

    单个高斯模型的公式为
    $$g(x,\mu,\sigma)=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{-(x-\mu)^2}{2\sigma^2}} $$

    混合高斯模型：多个高斯模型同时存在的混合模型，$\alpha_i\rightarrow高斯模型的权重 $
    $$f(x)=\sum^n_{i=1}\alpha_ig(x,\mu_i,\sigma_i)\quad\alpha_i\geq0\&\sum_i\alpha_i=1 $$
## Expectation Maximization with Gaussian Mixture
假定一维高斯
* 符号

    $$m\longrightarrow数据点的个数 $$
    $$n\longleftrightarrow高斯模型的个数 $$
    $$z_{ij}\longrightarrow命题：第i个样本点是否是被第j个高斯模型生成的 $$
* 过程描述

    1. 假定所有高斯模型的均值和权重 

        $$\mu_j和\alpha_j\quad 0\leq j\leq n $$
    2. 计算每个样本的期望(贝叶斯公式：执果索因)

        $$E|z_{ij}|=\frac{p(x=x_i|\mu=\mu_j)\alpha_j}{\sum_{k=1}^{n}p(x=x_i|\mu=\mu_k)\alpha_k}\\
        =\frac{e^{-\frac{-(x_i-\mu_j)^2}{2\sigma^2}}\alpha_j}{\sum^n_{k=1}e^{-\frac{-(x_i-\mu_k)^2}{2\sigma^2}}\alpha_k}$$
    3. 依据期望重设参数

        $$\mu_j\longleftarrow\frac{\sum_{i=1}^mE|z_{ij}|x_i}{\sum_{i=1}^mE|z_{ij}|} $$

        $$\alpha_j\longleftarrow\frac{1}{m}\sum_{i=1}^mE|z_{ij}| $$
    4. 返回i.

## Density Based Methods(DBSCAN)
* 概念

    1. 核心点Core Point
    2. 边缘点Border Point
    3. 噪点Noise Point
    4. directly density reachable
    5. density reachable
    6. density connected
* 过程描述

    <https://www.bilibili.com/video/BV154411Q7mG?p=39> 6:19
## hierarchical clustering(agglomerative methods)
找出所有聚类的可能结果，真正的聚类结果需要人为使用阈值分割
* 过程描述

    <https://www.bilibili.com/video/BV154411Q7mG?p=39>
    10:36
    
    如果是按最小欧式距离进行聚类的话，在思路上非常类似于力求WPL最小化的哈夫曼树构造法

    如果是按最大欧氏距离，其结果将会发生变化

    聚类之间的距离，由两个类内的成员决定，A类和B类的距离是A中点到B中点所有距离构成的集合中的最小值（按最小欧式距离聚类）