{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. torch.nn命名空间提供了构建自己的神经网络所需的所有构建块。\n",
    "2. PyTorch中的每个模块都是nn.module的子类。\n",
    "3. 神经网络是由其他模块（层）组成的模块本身。这种嵌套结构允许轻松构建和管理复杂的体系结构。\n",
    "\n",
    "在下面的部分中，我们将构建一个神经网络来对FashionMNIST数据集中的图像进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用torch.cuda is available判断是否可用GPU加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过对nn.Module进行子类化来定义我们的神经网络。模块，并在__init__中初始化神经网络层。\n",
    "\n",
    "每个nn.Module子类在forward方法中实现对输入数据的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们创建了NeuralNetwork的一个实例，并将其移动到设备上，然后打印其结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用模型，我们将输入数据传递给它。这将执行模型的forward以及一些后台操作。\n",
    "\n",
    "在输入上调用模型会返回一个二维张量，dim=0对应于每个类的10个原始预测值的每个输出，dim=1对应于每个输出的单个值。我们通过将其传递给nn.Softmax模块的一个实例来获得预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([5], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们来分解FashionMNIST模型中的层。为了说明这一点，我们将使用3张尺寸为28x28的图片作为样本，看看在通过网络时会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们初始化nn.Flatten以将每个2D 28x28图像转换为784个像素值的连续阵列（保持小批量尺寸（dim=0））。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性层nn.Linear是使用其存储的权重和偏差对输入应用线性变换的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非线性激活是在模型的输入和输出之间创建复杂映射的原因。它们在线性变换后被应用于引入非线性，帮助神经网络学习各种现象。\n",
    "\n",
    "使用ReLUJ激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.0466, -0.1624,  0.4153, -0.0371, -0.3682,  0.1618,  0.3354,  0.0048,\n",
      "          0.2566, -0.6558,  0.2853, -0.2052, -0.1324,  0.3608,  0.2474, -0.0434,\n",
      "          0.0686, -0.5062,  0.3249,  0.2683],\n",
      "        [ 0.2245, -0.2510,  0.3030,  0.1618, -0.2358, -0.3639,  0.4615,  0.2633,\n",
      "          0.2690, -0.4320,  0.2208, -0.3254, -0.0795,  0.1691,  0.2043,  0.0842,\n",
      "          0.5477, -0.3101,  0.3475,  0.6510],\n",
      "        [ 0.1259, -0.0109,  0.1554,  0.3019, -0.3158, -0.0515,  0.3052, -0.0199,\n",
      "          0.1829, -0.5917,  0.1265, -0.1981,  0.1889,  0.1820,  0.2793,  0.0167,\n",
      "          0.4231, -0.1300,  0.0905,  0.5969]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0466, 0.0000, 0.4153, 0.0000, 0.0000, 0.1618, 0.3354, 0.0048, 0.2566,\n",
      "         0.0000, 0.2853, 0.0000, 0.0000, 0.3608, 0.2474, 0.0000, 0.0686, 0.0000,\n",
      "         0.3249, 0.2683],\n",
      "        [0.2245, 0.0000, 0.3030, 0.1618, 0.0000, 0.0000, 0.4615, 0.2633, 0.2690,\n",
      "         0.0000, 0.2208, 0.0000, 0.0000, 0.1691, 0.2043, 0.0842, 0.5477, 0.0000,\n",
      "         0.3475, 0.6510],\n",
      "        [0.1259, 0.0000, 0.1554, 0.3019, 0.0000, 0.0000, 0.3052, 0.0000, 0.1829,\n",
      "         0.0000, 0.1265, 0.0000, 0.1889, 0.1820, 0.2793, 0.0167, 0.4231, 0.0000,\n",
      "         0.0905, 0.5969]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential是有序的模块容器。数据按照定义的相同顺序传递给所有模块。\n",
    "\n",
    "您可以使用顺序容器来组合一个快速网络，如seq_modules。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 神经网络的最后一个线性层返回在[-infty，infty]范围中的logits-raw values\n",
    "* 这些值传递nn.Softmax模块。逻辑被缩放为值[0，1]，表示每个类别的模型预测概率。\n",
    "* dim参数指示值总和必须为1的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络中的许多层是参数化的，即具有在训练期间优化的相关权重和偏差。nn.Module子类自动跟踪模型对象内定义的所有字段，\n",
    "\n",
    "你可以使用模型的parameters()、named_parameters()方法访问所有模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : Parameter containing:\n",
      "tensor([[-0.0309, -0.0324,  0.0021,  ..., -0.0270, -0.0280, -0.0287],\n",
      "        [ 0.0345,  0.0199, -0.0039,  ..., -0.0196, -0.0219, -0.0317],\n",
      "        [ 0.0233,  0.0138, -0.0333,  ..., -0.0098, -0.0091, -0.0162],\n",
      "        ...,\n",
      "        [ 0.0145, -0.0125,  0.0080,  ..., -0.0067, -0.0314,  0.0231],\n",
      "        [-0.0310,  0.0182,  0.0298,  ..., -0.0282,  0.0204,  0.0044],\n",
      "        [-0.0140,  0.0235,  0.0018,  ..., -0.0145, -0.0124,  0.0333]],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([-3.1316e-02,  3.0153e-02,  1.5606e-02, -1.1095e-02, -3.1958e-02,\n",
      "        -8.9613e-03,  7.1967e-03,  4.5948e-03,  3.0889e-02, -3.0572e-02,\n",
      "        -3.1071e-02,  4.9030e-03,  2.6255e-02,  2.5520e-02,  2.2341e-02,\n",
      "         3.0770e-02, -1.4838e-02, -1.9500e-02, -2.6150e-02,  2.0186e-02,\n",
      "        -3.5254e-02,  1.7486e-02, -4.7637e-03, -3.3314e-02, -1.5645e-02,\n",
      "         4.9380e-03, -1.9680e-02, -1.8171e-02,  3.5007e-02, -2.3252e-02,\n",
      "        -9.6906e-03,  5.2779e-03, -1.2501e-02, -3.5301e-02,  4.9955e-03,\n",
      "         2.4458e-02,  1.2919e-02, -2.2360e-03,  1.4969e-02, -6.6290e-03,\n",
      "         2.8768e-02,  1.5806e-02, -1.6783e-02, -7.4076e-03,  3.5194e-02,\n",
      "         2.7854e-02, -1.1195e-03,  3.1605e-02,  2.4107e-03, -1.0749e-02,\n",
      "         3.8515e-03,  5.2966e-03,  3.9933e-03,  1.1985e-02,  1.0834e-02,\n",
      "        -4.5420e-03,  3.3361e-02,  1.7617e-02, -3.2431e-02,  5.5249e-03,\n",
      "        -3.2372e-02,  6.9292e-03,  1.2701e-02,  7.4960e-03,  1.0353e-02,\n",
      "        -5.4371e-03,  9.5401e-03, -2.1294e-02, -2.8440e-03, -9.2761e-03,\n",
      "        -1.1156e-02, -1.6342e-02, -2.3407e-02, -3.0124e-02, -3.2763e-02,\n",
      "        -1.9267e-02,  1.2278e-02,  3.2930e-02,  2.6486e-02,  1.7903e-02,\n",
      "        -2.4885e-03,  1.6590e-02, -1.7991e-02,  1.4721e-02, -1.3942e-02,\n",
      "         2.6316e-02, -4.0889e-03, -3.8737e-03, -2.0910e-02,  2.9701e-02,\n",
      "         3.2815e-02,  2.7589e-02, -2.5040e-02, -2.5348e-02, -8.1410e-03,\n",
      "        -7.0344e-04,  4.9871e-03, -1.7133e-03,  7.3462e-03,  3.1891e-02,\n",
      "         3.5456e-03,  1.4510e-02,  3.3045e-02, -2.8193e-03, -2.1210e-02,\n",
      "        -2.4844e-02, -2.3305e-02, -2.2828e-02,  9.4309e-03, -2.6335e-02,\n",
      "        -1.7366e-02, -3.3327e-03, -1.5733e-02,  2.4796e-02, -1.1698e-02,\n",
      "        -2.7383e-02, -4.6846e-03, -4.2387e-04, -1.7831e-02, -2.4805e-02,\n",
      "        -1.8389e-02, -3.3223e-02,  1.5986e-02, -4.8876e-03, -7.1085e-03,\n",
      "        -2.3953e-02, -2.1264e-02,  2.4910e-02, -2.8268e-02,  2.8076e-02,\n",
      "         3.2032e-02, -7.1017e-03,  1.8411e-03, -1.5949e-02, -1.7643e-03,\n",
      "        -2.5941e-02,  2.3830e-02,  2.2329e-02,  2.3621e-02, -1.7718e-02,\n",
      "         7.6325e-03, -2.5161e-02, -1.3176e-02,  5.3113e-03,  2.9091e-02,\n",
      "         2.0977e-04,  1.1186e-02,  1.3930e-02,  1.3360e-02, -1.7283e-02,\n",
      "         3.5647e-02, -2.0614e-02, -2.4624e-02, -1.3539e-02, -5.3640e-03,\n",
      "         2.4347e-02, -1.3633e-02,  7.2265e-04, -5.1312e-03, -3.5315e-02,\n",
      "         3.4377e-02,  2.3482e-02, -3.2488e-02, -2.1476e-02,  3.5689e-02,\n",
      "         1.1441e-02,  4.4764e-03, -5.7636e-03, -2.0082e-02, -2.8987e-02,\n",
      "         1.6744e-02,  2.3272e-02,  2.4006e-02,  2.2243e-02,  1.3976e-02,\n",
      "        -3.0363e-02, -1.1607e-02,  5.6010e-03, -1.2007e-02, -5.5647e-03,\n",
      "        -9.5830e-03,  2.4301e-02, -1.6827e-02,  3.2620e-02, -5.8608e-03,\n",
      "        -3.4958e-02,  1.1156e-03,  3.0598e-04,  1.5156e-02,  2.7425e-02,\n",
      "         1.9285e-02,  8.4125e-04, -1.9381e-03,  1.6178e-02,  2.7486e-02,\n",
      "        -7.6239e-03,  5.7856e-03, -9.7162e-03, -3.2873e-02,  1.7797e-02,\n",
      "        -2.4878e-02,  5.2949e-03,  3.4395e-02, -1.0807e-02,  3.3081e-02,\n",
      "         3.2701e-02,  1.5086e-02, -8.6067e-04,  2.4351e-02,  1.1120e-02,\n",
      "        -9.4986e-03,  1.3370e-02,  7.6683e-03, -8.0261e-03, -2.3111e-02,\n",
      "         1.9582e-02,  1.4528e-02, -2.6093e-02,  2.3231e-03,  7.5132e-03,\n",
      "         1.7232e-03,  1.8215e-02,  8.7971e-03, -1.8249e-02,  1.1401e-02,\n",
      "         2.7912e-02, -1.6418e-02, -2.0229e-02,  2.5961e-02,  2.5158e-02,\n",
      "         2.5923e-02,  3.3538e-02, -1.0162e-02, -1.3403e-02, -1.4656e-02,\n",
      "        -3.5523e-02,  5.7155e-03,  1.1053e-02,  2.2492e-02,  9.2020e-03,\n",
      "        -7.6668e-03, -3.5536e-02, -1.4834e-02, -9.1530e-03,  3.0812e-02,\n",
      "         2.9202e-02,  1.0797e-02, -2.2815e-02, -1.1659e-02,  2.2746e-04,\n",
      "         2.1660e-02,  2.5769e-03,  1.4389e-02,  1.8326e-02,  1.2607e-02,\n",
      "        -1.1172e-02, -1.1218e-03, -1.3858e-02,  4.6409e-03, -1.1413e-02,\n",
      "        -1.9169e-02, -2.8736e-02,  8.3809e-03, -1.8007e-02, -3.2160e-02,\n",
      "        -1.1724e-02, -1.8281e-03,  2.1436e-02,  3.2281e-02, -1.9852e-02,\n",
      "        -1.4820e-02, -9.9211e-03, -2.4649e-02, -1.5537e-02, -1.6255e-02,\n",
      "        -1.3860e-02, -1.0500e-02, -2.8867e-02,  3.1694e-02,  1.6576e-02,\n",
      "         2.9877e-02, -2.9925e-02,  2.4063e-02, -6.6930e-03,  3.5698e-02,\n",
      "        -8.4976e-03,  1.4291e-02, -8.0130e-03,  6.3445e-03,  2.3281e-02,\n",
      "         5.0317e-03, -7.3597e-03,  8.2915e-03,  1.4608e-02,  1.9091e-02,\n",
      "        -1.0149e-02, -2.5424e-02, -3.0355e-02,  3.4663e-02, -1.6633e-02,\n",
      "        -1.7777e-02, -2.1655e-04, -2.8233e-02,  1.0161e-02, -4.9592e-03,\n",
      "        -1.7090e-02, -1.8947e-02,  7.9685e-03,  3.1948e-03, -3.5294e-02,\n",
      "         2.5888e-02, -2.1661e-02,  2.6088e-02,  3.4596e-02,  1.9550e-02,\n",
      "         2.4778e-02, -3.2089e-02,  3.5583e-02, -9.7408e-03,  9.6222e-03,\n",
      "         3.1427e-02,  1.6713e-02,  1.3483e-02,  1.9087e-02, -2.8084e-03,\n",
      "         6.3090e-03, -2.1543e-04, -2.4362e-02, -2.9238e-02, -1.2661e-03,\n",
      "        -7.1483e-03,  7.2183e-03,  3.2771e-02,  4.5117e-03, -1.0790e-02,\n",
      "         1.7482e-03, -1.1413e-02,  2.3357e-02,  1.6124e-02,  3.4926e-02,\n",
      "         2.8041e-02, -2.6436e-03,  3.3249e-02,  8.5492e-04,  2.0915e-02,\n",
      "        -5.4164e-03,  1.8828e-02,  2.0706e-02, -1.8443e-02,  1.2208e-02,\n",
      "         7.5429e-03,  1.7125e-02,  3.5527e-02, -3.5900e-03,  1.7795e-02,\n",
      "         1.4212e-02,  3.1609e-02,  2.1467e-03,  1.2764e-02,  3.3625e-03,\n",
      "        -2.8527e-02, -1.4082e-02, -4.5783e-03,  1.4178e-02,  3.1252e-02,\n",
      "        -1.1656e-02, -1.4823e-02,  5.2606e-03,  1.4806e-02, -1.8349e-02,\n",
      "        -1.9564e-02,  4.5342e-03, -4.6018e-03, -9.6704e-03,  2.5772e-02,\n",
      "         1.4895e-02, -5.5737e-03, -3.2792e-02,  2.6983e-02,  1.4754e-02,\n",
      "        -2.1495e-02, -1.1348e-02,  8.4201e-03, -3.2731e-02,  1.0566e-02,\n",
      "         2.2149e-02,  3.2852e-03,  1.5157e-02, -3.4024e-03, -2.4264e-02,\n",
      "        -1.8390e-02,  1.9813e-02, -2.5976e-02,  4.1144e-04,  3.1896e-02,\n",
      "         7.3377e-03,  2.1122e-02, -1.2294e-02, -1.3677e-02,  5.5129e-03,\n",
      "         3.0039e-03, -2.8582e-02,  1.7536e-02,  2.5459e-02, -2.7812e-03,\n",
      "         2.7522e-02,  2.4988e-03, -5.1213e-03,  1.0147e-03, -3.4970e-02,\n",
      "         2.7723e-02,  1.0974e-02, -4.8357e-03, -3.5160e-02, -1.6430e-02,\n",
      "        -2.5394e-03, -3.1000e-02, -3.4764e-02, -3.0315e-02,  1.1508e-02,\n",
      "        -1.7008e-02, -7.8619e-03,  3.4738e-02,  2.8933e-02,  1.2788e-02,\n",
      "         1.3734e-02,  2.0157e-02,  2.9686e-02,  1.8472e-02, -1.3939e-02,\n",
      "         3.4160e-02, -1.3133e-02,  2.0967e-02, -3.2621e-03, -3.5626e-02,\n",
      "         2.8536e-02, -1.3388e-02,  1.1840e-02, -2.3454e-02,  3.4137e-02,\n",
      "         2.4686e-02, -7.5527e-03,  2.2970e-02, -2.2256e-02,  3.0715e-02,\n",
      "        -8.3683e-03,  2.6754e-02, -1.5959e-03, -8.4250e-03,  2.2808e-02,\n",
      "         1.2298e-02, -3.3616e-02,  1.7625e-02,  1.4200e-02,  2.5961e-02,\n",
      "        -2.4939e-02,  2.0046e-02,  3.1675e-02, -2.4692e-02,  2.7989e-02,\n",
      "         2.4598e-02, -4.1267e-03, -1.7612e-02,  9.6840e-03,  3.4203e-03,\n",
      "        -2.9043e-02,  3.8519e-03,  1.2335e-02, -2.4153e-02, -3.4473e-02,\n",
      "        -9.3433e-03,  1.8236e-02,  1.7149e-02,  8.7250e-05, -2.9557e-02,\n",
      "         6.9254e-03,  1.2803e-02,  4.1124e-03,  2.0959e-02, -1.4835e-02,\n",
      "         1.1734e-03,  1.5945e-02,  2.5183e-02,  1.4896e-02,  1.6185e-02,\n",
      "        -6.7954e-03,  1.7684e-02, -3.3676e-02,  7.2365e-03,  1.5443e-02,\n",
      "         2.7739e-02,  2.1064e-02, -1.0355e-02,  1.2771e-02,  2.4161e-02,\n",
      "         3.3121e-02,  1.8618e-02,  1.5939e-03, -2.9181e-03,  1.2927e-02,\n",
      "        -7.8344e-04, -2.1998e-02, -7.1551e-03,  3.2868e-02,  2.9661e-03,\n",
      "        -1.0262e-02, -3.0536e-03, -8.3856e-03, -2.8303e-02, -3.1879e-02,\n",
      "        -1.5096e-03, -3.3052e-03], device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : Parameter containing:\n",
      "tensor([[ 0.0431, -0.0347, -0.0224,  ..., -0.0415, -0.0421,  0.0440],\n",
      "        [-0.0138, -0.0147, -0.0436,  ..., -0.0151, -0.0262, -0.0314],\n",
      "        [ 0.0424, -0.0202,  0.0326,  ...,  0.0112, -0.0235, -0.0264],\n",
      "        ...,\n",
      "        [-0.0324,  0.0178,  0.0222,  ...,  0.0182, -0.0418,  0.0015],\n",
      "        [-0.0141, -0.0199,  0.0212,  ...,  0.0039,  0.0155, -0.0199],\n",
      "        [-0.0251, -0.0259, -0.0359,  ...,  0.0125, -0.0121, -0.0427]],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([ 0.0029,  0.0319,  0.0440,  0.0430,  0.0375,  0.0236, -0.0347,  0.0188,\n",
      "        -0.0313, -0.0391, -0.0171,  0.0098, -0.0089, -0.0109,  0.0386,  0.0159,\n",
      "         0.0012,  0.0388,  0.0161, -0.0267, -0.0126,  0.0124, -0.0241,  0.0408,\n",
      "        -0.0125,  0.0183,  0.0334,  0.0014, -0.0048, -0.0404, -0.0385, -0.0128,\n",
      "        -0.0138, -0.0213, -0.0099,  0.0206,  0.0377, -0.0375,  0.0276, -0.0228,\n",
      "         0.0102, -0.0004, -0.0159,  0.0330,  0.0083,  0.0077, -0.0395, -0.0044,\n",
      "         0.0150,  0.0332,  0.0156,  0.0093, -0.0020,  0.0171, -0.0121,  0.0215,\n",
      "         0.0231, -0.0027, -0.0146,  0.0069, -0.0230,  0.0125, -0.0050, -0.0344,\n",
      "        -0.0130, -0.0207,  0.0035, -0.0216,  0.0426, -0.0309, -0.0061, -0.0252,\n",
      "         0.0433, -0.0362,  0.0136,  0.0373,  0.0350, -0.0337, -0.0375, -0.0064,\n",
      "         0.0345, -0.0003, -0.0182,  0.0166, -0.0331,  0.0320,  0.0310, -0.0092,\n",
      "         0.0084, -0.0226,  0.0250,  0.0300,  0.0051, -0.0276, -0.0172,  0.0016,\n",
      "         0.0058,  0.0316, -0.0101,  0.0261, -0.0010,  0.0045, -0.0117, -0.0264,\n",
      "         0.0003, -0.0345,  0.0160, -0.0023, -0.0057,  0.0094, -0.0028, -0.0362,\n",
      "        -0.0412,  0.0392, -0.0194,  0.0384,  0.0402, -0.0117, -0.0067, -0.0268,\n",
      "         0.0039, -0.0168,  0.0182, -0.0195, -0.0028, -0.0188, -0.0320, -0.0036,\n",
      "        -0.0008,  0.0123, -0.0165,  0.0130, -0.0365, -0.0234,  0.0069,  0.0097,\n",
      "         0.0263,  0.0364,  0.0206, -0.0005,  0.0190,  0.0374, -0.0307, -0.0229,\n",
      "        -0.0211, -0.0238, -0.0142,  0.0067, -0.0246, -0.0082,  0.0332, -0.0361,\n",
      "        -0.0170,  0.0317, -0.0096,  0.0159,  0.0436, -0.0411, -0.0187, -0.0237,\n",
      "        -0.0370,  0.0327,  0.0184,  0.0437,  0.0291,  0.0331,  0.0307, -0.0262,\n",
      "         0.0417,  0.0276,  0.0315,  0.0367,  0.0170,  0.0334,  0.0375,  0.0321,\n",
      "         0.0218, -0.0295,  0.0261,  0.0088,  0.0306, -0.0113, -0.0406,  0.0127,\n",
      "         0.0259, -0.0415, -0.0295,  0.0071, -0.0367,  0.0400,  0.0316,  0.0428,\n",
      "        -0.0247, -0.0348, -0.0026, -0.0002, -0.0108,  0.0326, -0.0133,  0.0372,\n",
      "        -0.0280,  0.0369,  0.0405, -0.0329, -0.0177,  0.0418, -0.0217,  0.0395,\n",
      "         0.0376, -0.0249, -0.0412,  0.0235, -0.0122,  0.0350,  0.0082,  0.0341,\n",
      "         0.0351, -0.0363, -0.0380, -0.0286, -0.0320,  0.0063,  0.0247,  0.0351,\n",
      "        -0.0349, -0.0381, -0.0240,  0.0336, -0.0165, -0.0441, -0.0290, -0.0272,\n",
      "         0.0097,  0.0239,  0.0366,  0.0064, -0.0361,  0.0432,  0.0261, -0.0326,\n",
      "         0.0037,  0.0236,  0.0321,  0.0093, -0.0259, -0.0221,  0.0063,  0.0223,\n",
      "        -0.0203, -0.0362, -0.0065, -0.0014,  0.0238, -0.0381, -0.0014, -0.0438,\n",
      "        -0.0407,  0.0308,  0.0398,  0.0238,  0.0301,  0.0327,  0.0366,  0.0234,\n",
      "         0.0083, -0.0384,  0.0076,  0.0425, -0.0125,  0.0117, -0.0050, -0.0312,\n",
      "        -0.0238, -0.0138,  0.0098,  0.0238,  0.0323,  0.0350, -0.0433, -0.0232,\n",
      "        -0.0278, -0.0217,  0.0017,  0.0067, -0.0192,  0.0305, -0.0132,  0.0434,\n",
      "        -0.0154, -0.0185,  0.0104,  0.0195,  0.0079, -0.0176,  0.0297,  0.0206,\n",
      "        -0.0269,  0.0184,  0.0297, -0.0151, -0.0098,  0.0159,  0.0227, -0.0005,\n",
      "        -0.0131, -0.0271,  0.0335,  0.0397, -0.0415,  0.0040,  0.0146,  0.0116,\n",
      "        -0.0365, -0.0382,  0.0067, -0.0121,  0.0207,  0.0404, -0.0247,  0.0223,\n",
      "         0.0385, -0.0152, -0.0213,  0.0283, -0.0274,  0.0434, -0.0366, -0.0126,\n",
      "        -0.0221, -0.0352, -0.0431,  0.0052,  0.0108, -0.0037,  0.0417, -0.0054,\n",
      "        -0.0058, -0.0179, -0.0238,  0.0269, -0.0330,  0.0305,  0.0267,  0.0138,\n",
      "        -0.0238,  0.0162,  0.0078,  0.0182,  0.0154, -0.0125,  0.0166,  0.0233,\n",
      "         0.0363, -0.0234, -0.0416, -0.0386,  0.0174,  0.0383, -0.0226,  0.0055,\n",
      "         0.0409,  0.0147, -0.0278,  0.0201, -0.0070, -0.0012, -0.0176,  0.0277,\n",
      "         0.0194, -0.0216, -0.0035,  0.0408, -0.0059, -0.0408,  0.0070,  0.0228,\n",
      "         0.0017,  0.0059,  0.0038, -0.0437, -0.0197,  0.0044,  0.0215, -0.0376,\n",
      "        -0.0429, -0.0196,  0.0155,  0.0322,  0.0232, -0.0088, -0.0297,  0.0422,\n",
      "        -0.0096, -0.0335,  0.0358, -0.0031,  0.0387,  0.0187,  0.0268,  0.0002,\n",
      "         0.0018, -0.0225, -0.0242, -0.0206, -0.0393,  0.0401, -0.0442, -0.0054,\n",
      "        -0.0245,  0.0137, -0.0237,  0.0300,  0.0292, -0.0116,  0.0022,  0.0023,\n",
      "         0.0436,  0.0392, -0.0130, -0.0103,  0.0311,  0.0284,  0.0359, -0.0054,\n",
      "        -0.0357, -0.0047,  0.0230, -0.0363, -0.0156,  0.0418,  0.0007,  0.0246,\n",
      "         0.0158, -0.0238, -0.0296, -0.0181, -0.0339, -0.0408, -0.0036,  0.0043,\n",
      "        -0.0160, -0.0382,  0.0369, -0.0156, -0.0149, -0.0133, -0.0357,  0.0021,\n",
      "        -0.0035,  0.0233, -0.0214,  0.0088,  0.0161,  0.0079,  0.0275,  0.0158,\n",
      "        -0.0342, -0.0024, -0.0339, -0.0403,  0.0235, -0.0090, -0.0273,  0.0409,\n",
      "         0.0226,  0.0334,  0.0257, -0.0346, -0.0288, -0.0187,  0.0273,  0.0320,\n",
      "        -0.0148, -0.0151,  0.0391,  0.0138,  0.0175,  0.0425,  0.0250, -0.0411,\n",
      "        -0.0268, -0.0235,  0.0349,  0.0403,  0.0072,  0.0195, -0.0275, -0.0295,\n",
      "        -0.0109, -0.0073, -0.0406, -0.0366,  0.0101, -0.0371,  0.0144, -0.0292,\n",
      "        -0.0350, -0.0016,  0.0357, -0.0346, -0.0209,  0.0143,  0.0309, -0.0296,\n",
      "        -0.0383,  0.0388,  0.0331, -0.0382, -0.0049, -0.0371,  0.0159,  0.0174],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : Parameter containing:\n",
      "tensor([[-0.0257, -0.0044, -0.0360,  ..., -0.0338, -0.0167, -0.0080],\n",
      "        [-0.0365, -0.0194, -0.0325,  ...,  0.0040, -0.0263, -0.0402],\n",
      "        [-0.0341, -0.0063,  0.0325,  ...,  0.0377, -0.0105, -0.0303],\n",
      "        ...,\n",
      "        [-0.0416, -0.0106, -0.0239,  ..., -0.0396,  0.0233, -0.0355],\n",
      "        [ 0.0123,  0.0061, -0.0093,  ...,  0.0113, -0.0162,  0.0353],\n",
      "        [ 0.0138,  0.0005, -0.0189,  ...,  0.0188,  0.0353,  0.0247]],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : Parameter containing:\n",
      "tensor([-0.0172, -0.0009, -0.0025,  0.0013,  0.0411,  0.0128, -0.0171, -0.0392,\n",
      "        -0.0238, -0.0206], device='cuda:0', requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f76d4fda128b12615e46e0e8dd834a222e7abd956eb53de74309670d1db4104c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
