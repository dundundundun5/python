# Artificial Neural Networks
输入层、隐藏层、输出层
## 基础
1. perceptrons感知机

    感知机的输入是输入层和权重点积，w0
    
    
    输出是激活函数(阶跃函数)作用下的输出

    梯度下降用于逐渐缩小误差直到最小
    
    梯度下降，实质是让误差对每个权重分别求偏导

    批量梯度下降，是对每个样本都求一次偏导再求和求平均

    stochastic learning 仅仅是deltaw变了


    误差大了，就要减小权重，因此deltaw是负数

    默认是线性加权，所以

    感知机难以处理线性不可分问题
2. 多层感知机（ANN）

    多层感知机可以解决线性不可分问题

    XOR = -(p and q) and (p or q) 

    and(nand ,or)
    如果从nand 和or的角度，数字再一次被分类了，隐藏层的分类更容易被解析

    sigmoid函数保证输出位于(0, 1)，且曲线处处平滑，全域可微
3. 多层感知机的BP算法（BP网络）

    基本解决了从后向前根据输出调整边权的问题
    $$x_{ji}\rightarrow 第j个神经元的第i个输入$$
    $$w_{ji}\rightarrow  第j个神经元的第i个输入的权重$$
    $$net_j=\sum{w_{ji}x_{ji}}  $$
    $$downstream(j)\rightarrow把第j个神经元输出作为输入的神经元集合  $$
    $$\sigma\rightarrow sigmoid函数 $$
    $$ o_j\quad t_j\longleftarrow第j个神经元的计算输出和实际输出 $$
    Bp算法的大概步骤是

    1. 创建一个神经网络，具有不同数量的输入层、隐藏层、输出层
    2. 将所有边权初始化为较小的随机数字
    3. 在误差到达极小之前，对每个训练集的输入输出
    
        1. 计算每个神经元的输出
        2. 对每个输出神经元k，计算误差项
        3. 对每个隐藏神经元h，计算其误差项
        4. 更新网络的边权 
    * 输出层的输入权重调整方法，使用的是梯度下降原则

    * 隐藏层的输入权重调整方法，将收到输出层的输出误差影响

    * 推导过程很难，略

    * 梯度下降容易掉入局部极小值，所以权重可以设置不同的起始位置
    
    * 在缩小误差的过程中，除了调整学习率以外，还加入了冲量（momentum）的概念
4. 其他算法

    Elman network 只有一个输入，但是能记忆上一个时刻的输入

    hotfield network 一个全互联网络，可以记忆一个模式（pattern），通过训练，可以让每个模式处于整个网络能量的极小值
5. ANN的优缺点

    训练很慢，使用很快

    过程很难解释，最直观的只有输入和输出