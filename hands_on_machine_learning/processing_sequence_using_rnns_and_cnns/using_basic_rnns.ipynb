{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        batch_size:批处理大小\n",
    "        n_steps: 时间步长\n",
    "    return: \n",
    "        一个形状为[batch_size, n_steps, 1]的列表\n",
    "        产生单变量时间序列,由两个固定振幅但频率和相位随机的正弦波总和组成\n",
    "    \"\"\"\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    # 在[0,1]创建等差数列，共n_steps个数字\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    # wave 1\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
    "    # wave 2\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))\n",
    "    # +noise\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    # +dimension 输入特征通常为3D数组\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "    # it works when not adding 1d to series\n",
    "    # return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单变量时间序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个时间序列有51个值\n",
    "# 解释一下，series[示例号，时间序列号]\n",
    "# X 包含 许多个从0时刻到n_steps-1时刻的时间序列\n",
    "# Y 包含 许多个n_steps时刻的值\n",
    "# X获取索引为0~49共50个时刻的时间序列，y获取索引为50时刻为51的时间序列\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只用简单的线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50, 1]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "# 使用MSE损失和Adam优化器编译并在训练集上训练\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后在验证集上评估\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一个简单的RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用深度RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让简单RNN层只保留最后一个输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不会使用最终层的隐藏状态\n",
    "# 由于可能使用别的激活函数，则最好换成密集层\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "用于一次性预测未来10步的时间序列（代码略）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于每一个时刻都预测未来10步的时间序列（下为代码）\n",
    "\n",
    "时刻0预测 1~10， 时刻1预测 2~11，时刻49预测 50~59\n",
    "\n",
    "每当模型作预测的时候，模型都只能看到过去时刻的输入，而看不到未来，所以尽管数据集的每个实例每个时间序列对应值都是预设的，模型并不会因此而受到影响\n",
    "\n",
    "<font color='yellow'>呵呵</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train = series[:7000, :n_steps]\n",
    "X_valid = series[7000:9000, :n_steps]\n",
    "X_test = series[9000:, :n_steps]\n",
    "Y = np.empty((10000, n_steps, 10))\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[..., step_ahead - 1] = series[..., step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自定义指标\n",
    "# 虽然一次预测十步，\n",
    "# 但还是只用每个时间序列的最后时刻的输出作为评估数据\n",
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用批量归一化（略）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义记忆单元\n",
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        # 获取没有激活函数的SimpleRNNCell\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
    "        # 获取所需的归一化（此处是层归一化）\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        # 获取所需的激活函数\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    # 应用于简单的RNN单元\n",
    "    def call(self, inputs, states):\n",
    "        # 计算当前输入和先前隐藏状态的线性组合，并返回两个结果\n",
    "        # new_states[0]等于outputs\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        # 进行层归一化后再激活\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        # 一个作为输出，一个是新的隐藏状态\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有循环层和keras提供的所有单元都有一个dropout和recurrent_dropout超参数\n",
    "\n",
    "前者用于每个时间步长的输入的dropout率，后者定义了隐藏装他的dropout率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要使用自定义单元，我们需要做的是创建一个keras.layers.RNN层，并向其传递一个单元实例\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
    "                     input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>解决短期记忆问题</font>\n",
    "\n",
    "<font color='blue'>如果你把LSTM单元视为黑匣子，那就等于没学</font>\n",
    "\n",
    "https://blog.csdn.net/shijing_0214/article/details/52081301\n",
    "\n",
    "https://blog.csdn.net/niuxuerui11/article/details/109036092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 8s 16ms/step - loss: 0.0774 - last_time_step_mse: 0.0610 - val_loss: 0.0553 - val_last_time_step_mse: 0.0338\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.0495 - last_time_step_mse: 0.0268 - val_loss: 0.0465 - val_last_time_step_mse: 0.0252\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 3s 14ms/step - loss: 0.0419 - last_time_step_mse: 0.0198 - val_loss: 0.0390 - val_last_time_step_mse: 0.0176\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0369 - last_time_step_mse: 0.0158 - val_loss: 0.0350 - val_last_time_step_mse: 0.0148\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0339 - last_time_step_mse: 0.0142 - val_loss: 0.0328 - val_last_time_step_mse: 0.0141\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0318 - last_time_step_mse: 0.0133 - val_loss: 0.0310 - val_last_time_step_mse: 0.0130\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0305 - last_time_step_mse: 0.0129 - val_loss: 0.0297 - val_last_time_step_mse: 0.0126\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0296 - last_time_step_mse: 0.0125 - val_loss: 0.0290 - val_last_time_step_mse: 0.0127\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0287 - last_time_step_mse: 0.0118 - val_loss: 0.0281 - val_last_time_step_mse: 0.0117\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0279 - last_time_step_mse: 0.0113 - val_loss: 0.0283 - val_last_time_step_mse: 0.0116\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0272 - last_time_step_mse: 0.0107 - val_loss: 0.0266 - val_last_time_step_mse: 0.0109\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0266 - last_time_step_mse: 0.0103 - val_loss: 0.0268 - val_last_time_step_mse: 0.0129\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0260 - last_time_step_mse: 0.0099 - val_loss: 0.0256 - val_last_time_step_mse: 0.0100\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0256 - last_time_step_mse: 0.0096 - val_loss: 0.0251 - val_last_time_step_mse: 0.0095\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.0251 - last_time_step_mse: 0.0092 - val_loss: 0.0249 - val_last_time_step_mse: 0.0093\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0247 - last_time_step_mse: 0.0090 - val_loss: 0.0245 - val_last_time_step_mse: 0.0095\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0244 - last_time_step_mse: 0.0089 - val_loss: 0.0240 - val_last_time_step_mse: 0.0090\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0241 - last_time_step_mse: 0.0087 - val_loss: 0.0242 - val_last_time_step_mse: 0.0091\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0238 - last_time_step_mse: 0.0086 - val_loss: 0.0235 - val_last_time_step_mse: 0.0091\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0237 - last_time_step_mse: 0.0086 - val_loss: 0.0231 - val_last_time_step_mse: 0.0085\n"
     ]
    }
   ],
   "source": [
    "# 简单地使用LSTM层而不是SimpleRNN层\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cd7ecbf46b3498f952f614292bd9b98de37d868a382baadab1fa3f5188ed6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
