{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从创建数据集开始，逐步研究如何构建Char-RNN\n",
    "\n",
    "并从Anderej Karpathy的CharRNN项目中下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Befor\n"
     ]
    }
   ],
   "source": [
    "# 使用keras.get_file()来下载莎士比亚的所有作品\n",
    "# shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = \"D:/temp_files/datasets/shakespeare/shakespeare.txt\"\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "print(shakespeare_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必须将每个字符编码为整数\n",
    "\n",
    "    使用keras的Tokenizer类，为文本添加一个分词器，它会找到文本中使用的所有字符，并将它们映射到不同的字符ID，从1开始（不从0，所以可用0进行屏蔽）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看有哪些字符\n",
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建分词器\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tokenizer类分词,char_level=True来得到字符集编码，而不是单词集编码\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)\n",
    "# 请注意，默认情况下，该分词器将文本转换为小写（def:lower=True）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text->sequences\n",
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequences->text\n",
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of distinct characters\n",
    "max_id = len(tokenizer.word_index)\n",
    "# total number of characters\n",
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>简而言之，将时间序列划分集合并不是一件容易的事，可能要按时间划分，也可能是在所有时间上分层抽样，如何划分是一门学问，要具体情况具体分析</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 将顺序数据集切成多个窗口\n",
    "\n",
    "<font color = \"yellow\">\n",
    "训练集现在由一个超过一百万字符的单个序列组成，如果直接输入并训练，就相当于产生了一个超过一百万层的深层网络，时间序列非常长\n",
    "</font>\n",
    "\n",
    "---\n",
    "<font color = \"green\">\n",
    "\n",
    "因此，使用window()将这个百万序列划分成许多小窗口\n",
    "\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n",
    "* 时间截断反向传播\n",
    "\n",
    "    数据集中的每个实例将是整个文本的很短的子字符串，并且RNN仅仅在这些子字符串的长度上展开\n",
    "\n",
    "### 基础划分举例\n",
    "\n",
    "    在开始之前，首先看一看如何把一个序列拆分成多个批次的随机洗牌窗口\n",
    "\n",
    "    把0~14，分成多个长度为5的窗口，每个窗口都左移两个单位，再对它们进行洗牌，最后把他们分成输入（除了尾值）和标签（除了头值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_steps = 5\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "# dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "# dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "# dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "# dataset = dataset.batch(3).prefetch(1)\n",
    "# for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "#     print(f\"Batch{index}: \")\n",
    "#     print(f\"X_Batch:\\n{X_batch.numpy()}\")\n",
    "#     print(f\"Y_Batch:\\n{Y_batch.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在数据集上切分\n",
    "\n",
    "    使用window()方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window()方法会创建不重叠的窗口，但如果使用shift=1,则窗口分布则会变成，0~100/1~101,\n",
    "\n",
    "如果使用drop_remainder=True,则丢弃了那些长度小于101的窗口，从而保证所有窗口长度一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "# target = input shifted 1 character ahead\n",
    "window_length = n_steps + 1 \n",
    "# 创建一个包含窗口的数据集，每个窗口也表示为一个数据集，列表的列表\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "# batch()方法将嵌套的长度不一的数据集，分为嵌套的长度均匀的数据集\n",
    "# 由于每个窗口长度相同，因此每个窗口都获得一个张量\n",
    "# flat_map()将嵌套的数据集转换为展平的数据集\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在得到了一个展平的数据集（张量），由于当训练集中的实例独立且同分布相同时，梯度下降效果最好，因此需要对这些窗口进行混洗\n",
    "\n",
    "然后，可以批处理(map)这些窗口并将输入和目标分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般应将类别输入特征，独热编码或者嵌入\n",
    "\n",
    "此处使用独热编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
    ")\n",
    "dataset = dataset.prefetch(1)\n",
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建和训练Char-RNN模型\n",
    "要基于前100个字符来预测下一个字符，我们可以使用两个有GRU层的RNN，每个GRU层有128个单元，输入和隐藏状态的dropout率均为20%\n",
    "\n",
    "输出层是一个时间分布的Dense层，这一次该层必须有max_id个单元，因为max_id表示文本中不同的字符数\n",
    "\n",
    "每个时间不长的输出概率总和为1，因此在输出层使用softmax\n",
    "\n",
    "训练时间可能非常久"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31368/31368 [==============================] - 588s 18ms/step - loss: 1.6200\n",
      "Epoch 2/10\n",
      "31368/31368 [==============================] - 576s 18ms/step - loss: 1.5388\n",
      "Epoch 3/10\n",
      "14716/31368 [=============>................] - ETA: 5:15 - loss: 1.5113"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')\n",
    "history = model.fit(dataset, epochs=10)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p279 保存模型的两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cd7ecbf46b3498f952f614292bd9b98de37d868a382baadab1fa3f5188ed6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
