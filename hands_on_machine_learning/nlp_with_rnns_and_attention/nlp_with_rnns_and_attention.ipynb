{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从创建数据集开始，逐步研究如何构建Char-RNN\n",
    "\n",
    "并从Anderej Karpathy的CharRNN项目中下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Befor\n"
     ]
    }
   ],
   "source": [
    "# 使用keras.get_file()来下载莎士比亚的所有作品\n",
    "# shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = \"D:/temp_files/datasets/shakespeare/shakespeare.txt\"\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "print(shakespeare_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必须将每个字符编码为整数\n",
    "\n",
    "    使用keras的Tokenizer类，为文本添加一个分词器，它会找到文本中使用的所有字符，并将它们映射到不同的字符ID，从1开始（不从0，所以可用0进行屏蔽）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看有哪些字符\n",
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建分词器\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tokenizer类分词,char_level=True来得到字符集编码，而不是单词集编码\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)\n",
    "# 请注意，默认情况下，该分词器将文本转换为小写（def:lower=True）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text->sequences\n",
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequences->text\n",
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of distinct characters\n",
    "max_id = len(tokenizer.word_index)\n",
    "# total number of characters\n",
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>简而言之，将时间序列划分集合并不是一件容易的事，可能要按时间划分，也可能是在所有时间上分层抽样，如何划分是一门学问，要具体情况具体分析</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 将顺序数据集切成多个窗口\n",
    "\n",
    "<font color = \"yellow\">\n",
    "训练集现在由一个超过一百万字符的单个序列组成，如果直接输入并训练，就相当于产生了一个超过一百万层的深层网络，时间序列非常长\n",
    "</font>\n",
    "\n",
    "---\n",
    "<font color = \"green\">\n",
    "\n",
    "因此，使用window()将这个百万序列划分成许多小窗口\n",
    "\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n",
    "* 时间截断反向传播\n",
    "\n",
    "    数据集中的每个实例将是整个文本的很短的子字符串，并且RNN仅仅在这些子字符串的长度上展开\n",
    "\n",
    "### 基础划分举例\n",
    "\n",
    "    在开始之前，首先看一看如何把一个序列拆分成多个批次的随机洗牌窗口\n",
    "\n",
    "    把0~14，分成多个长度为5的窗口，每个窗口都左移两个单位，再对它们进行洗牌，最后把他们分成输入（除了尾值）和标签（除了头值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_steps = 5\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "# dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "# dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "# dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "# dataset = dataset.batch(3).prefetch(1)\n",
    "# for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "#     print(f\"Batch{index}: \")\n",
    "#     print(f\"X_Batch:\\n{X_batch.numpy()}\")\n",
    "#     print(f\"Y_Batch:\\n{Y_batch.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在数据集上切分\n",
    "\n",
    "    使用window()方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window()方法会创建不重叠的窗口，但如果使用shift=1,则窗口分布则会变成，0~100/1~101,\n",
    "\n",
    "如果使用drop_remainder=True,则丢弃了那些长度小于101的窗口，从而保证所有窗口长度一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "# target = input shifted 1 character ahead\n",
    "window_length = n_steps + 1 \n",
    "# 创建一个包含窗口的数据集，每个窗口也表示为一个数据集，列表的列表\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "# batch()方法将嵌套的长度不一的数据集，分为嵌套的长度均匀的数据集\n",
    "# 由于每个窗口长度相同，因此每个窗口都获得一个张量\n",
    "# flat_map()将嵌套的数据集转换为展平的数据集\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在得到了一个展平的数据集（张量），由于当训练集中的实例独立且同分布相同时，梯度下降效果最好，因此需要对这些窗口进行混洗\n",
    "\n",
    "然后，可以批处理(map)这些窗口并将输入和目标分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般应将类别输入特征，独热编码或者嵌入\n",
    "\n",
    "此处使用独热编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
    ")\n",
    "dataset = dataset.prefetch(1)\n",
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建和训练Char-RNN模型\n",
    "要基于前100个字符来预测下一个字符，我们可以使用两个有GRU层的RNN，每个GRU层有128个单元，输入和隐藏状态的dropout率均为20%\n",
    "\n",
    "输出层是一个时间分布的Dense层，这一次该层必须有max_id个单元，因为max_id表示文本中不同的字符数\n",
    "\n",
    "每个时间不长的输出概率总和为1，因此在输出层使用softmax\n",
    "\n",
    "训练时间可能非常久"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31368/31368 [==============================] - 419s 13ms/step - loss: 1.6217\n",
      "Epoch 2/10\n",
      "31368/31368 [==============================] - 412s 13ms/step - loss: 1.5343\n",
      "Epoch 3/10\n",
      "31368/31368 [==============================] - 412s 13ms/step - loss: 1.5122\n",
      "Epoch 4/10\n",
      "31368/31368 [==============================] - 415s 13ms/step - loss: 1.5009\n",
      "Epoch 5/10\n",
      "31368/31368 [==============================] - 407s 13ms/step - loss: 1.4933\n",
      "Epoch 6/10\n",
      "31368/31368 [==============================] - 409s 13ms/step - loss: 1.4884\n",
      "Epoch 7/10\n",
      "31368/31368 [==============================] - 414s 13ms/step - loss: 1.4849\n",
      "Epoch 8/10\n",
      "31368/31368 [==============================] - 413s 13ms/step - loss: 1.4820\n",
      "Epoch 9/10\n",
      "31368/31368 [==============================] - 416s 13ms/step - loss: 1.4797\n",
      "Epoch 10/10\n",
      "31368/31368 [==============================] - 413s 13ms/step - loss: 1.4776\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')\n",
    "cb =  keras.callbacks.ModelCheckpoint(\"charRNN_cb.h5\")\n",
    "history = model.fit(dataset, epochs=10, callbacks=[cb])\n",
    "model.save(\"charRNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p279 保存模型的两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先加载保存好的模型\n",
    "model = keras.models.load_model(\"charRNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用CharRNN模型预测下一个字符，在提供文本前首先要对它进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = np.argmax(model(X_new), axis=-1)\n",
    "# 1st sentence, last char\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成假莎士比亚文本\n",
    "\n",
    "提供一些文本，让模型预测最可能的下一个字母，把它添加在文本末尾。然后以此循环，实际上效果不好：会反复出现相同的单词\n",
    "\n",
    "使用tf.random.categorical()函数估计的概率，随机选择下一个字符\n",
    "\n",
    "给定类对数概率/温度，函数会对随机类索引进行采样，接近0的温度倾向于高概率字符，而非常高的温度会给予所有字符相同的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成要添加到文本的下一个字符\n",
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char(\"How are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反复调用next_char()并添加到文本中\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the suitors to her father comes and with such serva'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text(\"t\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以添加更多的GRU层和每层有更多的神经元，训练更长时间并添加正则化\n",
    "\n",
    "当前模型无法学习比n_step更长的模式\n",
    "\n",
    "LSTM和GRU无法处理很长的序列，序列长度为100是一个临界值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有状态的RNN\n",
    "\n",
    "无状态RNN是每次训练迭代模型都从一个充满零的隐藏状态开始，<font color='aqua'>每个时间步长更新该状态，在最后一个时间步长之后将其丢弃</font>\n",
    "\n",
    "___如果让RNN在处理一个训练批次后保留这个最终状态并将其用作下一个训练批次的初始状态___ 这样，尽管反向传播只是通过短序列，模型仍可以学习长期模式，这就是有状态RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有当批次中的每个输入序列均从上一个批次中对应序列中断的确切位置开始时，有状态RNN才有意义，因此必须使用顺序和非重合的输入序列，而不能使用混洗的重合队列\n",
    "\n",
    "因此接下来使用包含单个窗口的批处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2,\n",
    "                     dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 12s 18ms/step - loss: 2.6234\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 2.2466\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 2.1155\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 2.0366\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.9855\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.9458\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.9200\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.8957\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.8790\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.8618\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.8486\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.8378\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.8272\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.8219\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.8121\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.8055\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7978\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.7885\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7873\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7817\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7769\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7721\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7680\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7672\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7611\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7582\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7550\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7504\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7491\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7463\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7408\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7405\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7391\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7358\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7354\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7328\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7309\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7263\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7274\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7263\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7241\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7221\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7213\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7193\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7158\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7161\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7127\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7120\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.7116\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.7122\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])\n",
    "model.save(\"stateful_RNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"stateful_RNN.h5\")\n",
    "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'complete_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(complete_text(\u001b[39m\"\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'complete_text' is not defined"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f76d4fda128b12615e46e0e8dd834a222e7abd956eb53de74309670d1db4104c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
