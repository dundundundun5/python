{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用特定初始化是让反向传播时，传播前和传播后，某层方差保持不变，从而缓解梯度爆炸和梯度消失的问题\n",
    "\n",
    "普通的sigmoid函数的平均值为0.5并非0，导致神经元输出的方差将大于输入的方差\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glorot 和 He 初始化\n",
    "[name for name in dir(keras.initializers) if not name.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|初始化|激活函数|$\\sigma^2$|\n",
    "|-|-|-|\n",
    "|Glorot|None,tanh,logistic,softmax|$\\frac{1}{fan_{avg}}$|\n",
    "|He|ReLU和变体|$\\frac{2}{fan_{in}}$|\n",
    "|LeCun|SELU|$\\frac{1}{fan_{in}}$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果使用均匀分布但基于fan_avg而不是fan_in\n",
    "# 进行He初始化，则可以使用Variance_Scaling初始化\n",
    "init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg'\n",
    "                                , distribution='uniform')\n",
    "        \n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比于ReLU，leaky ReLU能够防止加权和为负数时，神经元只会输出0导致神经元死亡\n",
    "$$ReLU(z)=\\max(0,z) $$\n",
    "\n",
    "$$leakyReLU(z)=\\max(\\alpha z,z) $$\n",
    "\n",
    "后续的，还有随机泄露ReLU（RReLU），在训练过程中在给定范围内随机选择 $\\alpha $\n",
    "\n",
    "有参数化泄露ReLU（PReLU）， $\\alpha $将在训练期间学习而不作为超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一种激活函数，指数线性单位（Exponential Linear Unit，ELU）\n",
    "\n",
    "$$\n",
    "ELU(z)=\n",
    "\\begin{cases}\n",
    "\n",
    "\\alpha(e^z - 1)\\quad if\\quad z<0\\\\\n",
    "z\\quad if\\quad z\\geq 0\n",
    "\n",
    "\n",
    "\\end{cases}\n",
    "$$\n",
    "如果 $\\alpha=1 $则函数所有位置都是平滑的，有助于梯度加速下降\n",
    "\n",
    "ELU的主要缺点时它的计算比ReLU及变体要慢\n",
    "\n",
    "SELU 可以使得网路是自归一化的，但是有条件p298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Batch Normaliztion批量归一化\n",
    "  \n",
    "    更换激活函数成功缓解了训练一开始时的梯度爆炸、消失，而无法保证训练一段时间后的旧疾复发\n",
    "\n",
    "    批量归一化通过让模型学习各层输入的最佳缩放和均值，从而缓解训练一段时间后的梯度爆炸、消失\n",
    "* 大致方法\n",
    "\n",
    "    在隐藏层的激活函数前，通过该层输入的均值和标准差的 **移动平均值**来估计训练期间的最终统计信息\n",
    "\n",
    "    1. 通过反向传播想学习输出缩放向量$\\gamma $和输出偏移向量$\\beta $\n",
    "    2. 学习使用指数移动平均值估计的最终的输入均值向量$\\mu $和最终输入标准差向量$\\sigma $\n",
    "* 具体实现\n",
    "\n",
    "    使用API实现非常简单直观，只需在每个隐藏层的激活函数之前或之后添加一个批量归一化的层\n",
    "\n",
    "    但是BN论文的作者主张在激活函数之前添加批量归一化层\n",
    "\n",
    "    首先是在激活后使用BN，随后是在激活前使用BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # 可选地在模型的第一层后添加一个BN\n",
    "    keras.layers.BatchNormalization(), \n",
    "    # 在激活函数后添加BN \n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个BN层的每个输入添加了四个参数\n",
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                    validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在试试在激活之前使用BN层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # 可选地在模型的第一层之后添加一个BN层\n",
    "    keras.layers.BatchNormalization(), \n",
    "    # 在激活函数之前使用BN， \n",
    "    # 在BN层之前的层不需要偏置项，因为BN自带偏置项\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向传播期间裁剪梯度，让它们永远不会超过某个阈值，是另一个缓解梯度爆炸的流行技术\n",
    "\n",
    "keras中仅仅需要设置clipvalue或clipnorm参数，即可实现梯度裁剪\n",
    "\n",
    "该优化器会将梯度向量的每个分量都蔡建伟-1.0到1.0之间的值，即将所有损失的偏导数限制在这个范围之间，阈值是可以调整的超参数\n",
    "\n",
    "应当通过设置clipnorm而不是clipvalue按照范数裁剪，确保梯度裁剪不会改变梯度的方向（按值裁剪、按范数裁剪）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import optimize\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用keras进行迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割fashion Mnist数据集，返回两个训练集\n",
    "# A训练集 X-标签不是凉鞋且不是衬衫的所有图片，y-X的标签列（多分类器）\n",
    "# B训练集 X-标签是凉鞋或衬衫的所有图片，y-X的标签是否为衬衫的标签列（二元分类器）\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "# 按类别分离Fashion Mnist的三大集合\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_A[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_B[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组件模型A，编译并训练\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=epochs,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "# 参考模型A的架构并训练一个新模型 \n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=epochs,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务B与任务A非常相似，上面的模型B通过模仿模型A的架构来完成训练\n",
    "\n",
    "现在也可以通过迁移学习来完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "# 获取除了输出层之前的所有层架构及其训练好的权重\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_A.layers[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在model_A和model_B_on_A共享一些层，对后者的训练也会影响前者的层权重\n",
    "\n",
    "如果想要分离控制二者，则必须使用api，clone_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于新的输出层是随机初始化的，可能存在较大的错误梯度，可能会破坏重用模型A的现成权重\n",
    "\n",
    "* 冻结训练\n",
    "\n",
    "    为了避免以上情况，就是在训练的前几个伦茨冻结重用的层，给新层一些时间来学习合理的权重\n",
    "* 代码\n",
    "\n",
    "    将每一层的可训练属性设置为False并编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 冻结重用的前几层（此处是除了是输出层以外的所有层）\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy',\n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=epochs,\n",
    "                            validation_data=(X_valid_B, y_valid_B))\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=epochs,\n",
    "                            validation_data=(X_valid_B, y_valid_B))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100 - 98.65) / (100 - 99.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试的精度高达98.58%, 错误率也降了1.59倍\n",
    "\n",
    "这么做实际上是“折磨数据直到信服位置”\n",
    "\n",
    "当论文看起来过于优秀时，你应该要怀疑：也许这个浮华的新技术实际上并没有多大的帮助（事实上，它甚至可能降低性能），但作者尝试了许多变体，仅报告了最好的结果（这可能是由于运气所致），而没有提及他们在途中遇到了多少次失败。\n",
    "\n",
    "在大多数情况下，这根本不是恶意的，但这是造成如此多科学结果永远无法复现的部分原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，迁移学习在小型密集网络上不能很好地工作，可能因为小型网络学习的模式很少，密集网络学习的是非常特定的模式，在其他任务中不是很有用\n",
    "\n",
    "迁移学习最适合深度卷积神经网络，该神经网络倾向于学习更为通用的特征检测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更快的优化器\n",
    "\n",
    "<font color='red'>公式中所有更新的变量均为向量</font>\n",
    "* 动量优化\n",
    "\n",
    "    动量优化非常关心先前的梯度（加速度概念），每次迭代都会从动量向量m中减去局部梯度，并通过添加该动量向量来更新权重\n",
    "\n",
    "    为了模拟摩擦机制并防止动量变得过大，引入超参数$\\beta $，另外$\\eta $还是学习率\n",
    "\n",
    "    $$m\\leftarrow\\beta m-\\eta\\nabla_{\\theta}J(\\theta) $$\n",
    "    $$\\theta\\leftarrow\\theta+m $$\n",
    "* Nesterov加速梯度\n",
    "\n",
    "    又称NNesterov动量优化，是动量优化的小变体，它不再局部位置$\\theta $，而在$\\theta+\\beta m处沿动量方向稍微提前处测量成本函数的梯度 $\n",
    "\n",
    "    $$m\\leftarrow\\beta m-\\eta\\nabla_{\\theta}J(\\theta+\\beta m) $$\n",
    "    $$\\theta\\leftarrow\\theta+m $$\n",
    "\n",
    "    这种小的调整有效是因为通常动量向量会指向正确的方向，因此使用在该方向上测得的更远的梯度而不是原始位置上的梯度会稍微准确一些\n",
    "\n",
    "* AdaGrad\n",
    "\n",
    "    通过沿着最陡峭的维度按比例缩小梯度向量\n",
    "\n",
    "    此处是逐个元素相乘，相除\n",
    "    $$s\\leftarrow s+ \\nabla_{\\theta}J(\\theta)\\times\\nabla_{\\theta}J(\\theta)$$\n",
    "    $$\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}J(\\theta) / \\sqrt{s+\\epsilon} $$\n",
    "\n",
    "    1. 将梯度的平方累计到向量s中\n",
    "    2. 梯度向量按比例因子$\\sqrt{s+\\epsilon} $缩小了，其中$\\epsilon是平滑项 $\n",
    "    \n",
    "    *该算法会降低学习率*，但是对于陡峭的维度，它的执行速度比对缓慢下降的维度的执行速度要快。这称为自适应学习率。好处是算法几乎不需要调整学习率超参数$\\eta$\n",
    "* RMSProp\n",
    "    \n",
    "    AdaGrad有可能下降太快，从而永远不能收敛到全局最优解。RMSProp解决了这个问题。\n",
    "\n",
    "    其通过只是累加最近迭代中的梯度（而不是自训练依赖的所有梯度）来解决这个问题\n",
    "\n",
    "    $$s\\leftarrow \\beta s+ (1-\\beta)\\nabla_{\\theta}J(\\theta)\\times\\nabla_{\\theta}J(\\theta)$$\n",
    "    $$\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}J(\\theta) / \\sqrt{s+\\epsilon} $$\n",
    "\n",
    "    衰减率$\\beta$是一个新的超参数，设置为0.9,通过施加衰减率相关的加权和，强调了最近迭代的梯度，弱化了梯度平方的权重从而避免像AdaGrad那样下降太快\n",
    "\n",
    "* Adam\n",
    "\n",
    "    结合了动量优化和RMSProp的思想，代表着自适应矩估计,(逐个元素相乘、逐个元素相除)\n",
    "    \n",
    "    1. $$m\\leftarrow\\beta_1 m-(1-\\beta_1)\\nabla_{\\theta}J(\\theta) $$ \n",
    "    \n",
    "    2. $$s\\leftarrow \\beta_2s+ (1-\\beta_2)\\nabla_{\\theta}J(\\theta)\\times\\nabla_{\\theta}J(\\theta)$$\n",
    "    3. $$\\hat m\\leftarrow\\frac{m}{1-\\beta^t_1} $$\n",
    "    4. $$\\hat s\\leftarrow\\frac{s}{1-\\beta_2^t} $$\n",
    "    5. $$\\theta\\leftarrow\\theta-\\eta\\hat m / \\sqrt{s+\\epsilon} $$\n",
    "\n",
    "    1、2、5与动量优化和RMSProp非常相似\n",
    "    3、4由于m和s初始化为0，因此在训练开始时它们会偏向0，这两个个步骤有助于在训练开始时提高m和s\n",
    "\n",
    "    Adam也是一种自适应学习率算法，因此对学习率超参数$\\eta$需要较少的调整\n",
    "\n",
    "* Nadam\n",
    "  \n",
    "    是在Adam优化上加上了Nesterov技巧，因此其收敛速度通常比Adam快\n",
    "\n",
    "**优化器比较P317**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练稀疏模型**\n",
    "\n",
    "在训练时使用强l1正规化，迫使优化器产生尽可能多的为0的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现动量优化, 0.9是一个动量可参考的超参值\n",
    "from re import T\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
    "# 实现Nesterov动量优化\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9, nesterov=True)\n",
    "# 不应该用AdaGrad\n",
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "# 使用RMSProp\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "# 使用Adam beta_1和beta_2的参数是实用的，并非随意给出\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "# 使用Nadam\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率调度P318\n",
    "* 幂调度\n",
    "\n",
    "    $$\\eta(t)=\\frac{\\eta_0}{(1+\\frac{t}{s})^c} $$\n",
    "    $$\\eta_0是初始学习率 \\quad t是迭代次数$$\n",
    "    $$s是每s步学习率下降一次，第i次下降将下降到\\frac{\\eta_0}{i} $$\n",
    "* 指数调度\n",
    "    $$\\eta(t)= \\eta_0 0.1^{\\frac{t}{s}}$$\n",
    "* 分段恒定调度\n",
    "* 性能调度\n",
    "* 1周期调度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用幂调度 decay参数是s的倒数\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过正则化避免过拟合 $\\ell_1$ and $\\ell_2$ regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2返回一个正则化函数，\n",
    "# 在训练过程中的每个步骤都将调用该正则化函数来计算正则化损失\n",
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过partial来简化函数内参数的书写\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam', metrics=['accuracy'])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2,\n",
    "                    validation_data=(X_valid_scaled, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropout技术\n",
    "\n",
    "    训练过程中，每个神经元都有p概率被删除\n",
    "\n",
    "    选取了<https://www.zhihu.com/people/li-kai-36-50-82>在<https://zhuanlan.zhihu.com/p/38200980>的评论\n",
    "    \n",
    "    说一下我的理解。首先假设一层神经网络中有n个神经元，其中一个神经元的输出是x，输出期望也是x。加上dropout后，有p的概率这个神经元失活，那么这个神经元的输出期望就变成了$(1-p)* x+p * 0=(1-p)x$，我们需要保证这个神经元在训练和测试阶段的输出期望基本不变。那么就有两种方式来解决：\n",
    "    \n",
    "        第一种在训练(按概率抛弃部分神经元)的时候，让这个神经元的输出缩放1/(1-p)倍，那么它的输出期望就变成(1-p)x/(1-p)+p*=x，和不dropout的输出期望一致；\n",
    "    \n",
    "        第二种方式是在测试(包含所有神经元)的时候，让神经元的输出缩放(1-p)倍，那么它的输出期望就变成了(1-p)x，和训练时的期望是一致的。\n",
    "    \n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* alpha dropout\n",
    "\n",
    "    他是dropout的一种变体，它保留了其输入的均值和标准差（常规的dropout会破坏自归一化）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MC dropout\n",
    "\n",
    "    该技术可以提高任何训练后的dropout模型的性能，提供更好地不确定性估计，而无需重新训练甚至不用修改它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Max norm\n",
    "\n",
    "    最大范数正则化，对于每个神经元，它会限制传入连接的权重w，使得$||w||_2\\leq r $，r是最大范数超参数\n",
    "\n",
    "    最大范数正则化不会把正则化损失项加入总体损失函数中，而是在每个训练步骤后计算并判断$||w||_2\\leq r $来实现。\n",
    "\n",
    "    每次训练迭代后，模型的fit()方法会调用由max_norm()返回的对象，将该层的权重传递给该对象，并获得返回的缩放权重，然后替换该层的权重\n",
    "\n",
    "    max_norm()的axis默认为0，意味着最大范数约束将独立应用于每个神经元的权重向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 总结\n",
    "\n",
    "    |超参数|默认值|\n",
    "    |-|-|\n",
    "    |内核初始化|He初始化|\n",
    "    |激活函数|ELU|\n",
    "    |归一化|浅层不需要、深层BN|\n",
    "    |正则化|提前停止（l2也行）|\n",
    "    |优化器|动量优化或RMSProp或Nadam|\n",
    "    |学习率调度|1周期|\n",
    "\n",
    "    如果网络是密集层的简单堆叠，则它可以自归一化，你应该使用表11-4中的配置。\n",
    "\n",
    "    |超参数|默认值|\n",
    "    |-|-|\n",
    "    |内核初始化|LeCun初始化|\n",
    "    |激活函数|SELU|\n",
    "    |归一化|不需要（自归一化）|\n",
    "    |正则化|如果需要：alpha dropout|\n",
    "    |优化器|动量优化或RMSProp或Nadam|\n",
    "    |学习率调度|1周期|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cd7ecbf46b3498f952f614292bd9b98de37d868a382baadab1fa3f5188ed6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
