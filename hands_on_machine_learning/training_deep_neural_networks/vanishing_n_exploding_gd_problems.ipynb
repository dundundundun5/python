{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用特定初始化是让反向传播时，传播前和传播后，某层方差保持不变，从而缓解梯度爆炸和梯度消失的问题\n",
    "\n",
    "普通的sigmoid函数的平均值为0.5并非0，导致神经元输出的方差将大于输入的方差\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glorot 和 He 初始化\n",
    "[name for name in dir(keras.initializers) if not name.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|初始化|激活函数|$\\sigma^2$|\n",
    "|-|-|-|\n",
    "|Glorot|None,tanh,logistic,softmax|$\\frac{1}{fan_{avg}}$|\n",
    "|He|ReLU和变体|$\\frac{2}{fan_{in}}$|\n",
    "|LeCun|SELU|$\\frac{1}{fan_{in}}$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1ef45924b50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1ef75383eb0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果使用均匀分布但基于fan_avg而不是fan_in\n",
    "# 进行He初始化，则可以使用Variance_Scaling初始化\n",
    "init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg'\n",
    "                                , distribution='uniform')\n",
    "        \n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比于ReLU，leaky ReLU能够防止加权和为负数时，神经元只会输出0导致神经元死亡\n",
    "$$ReLU(z)=\\max(0,z) $$\n",
    "\n",
    "$$leakyReLU(z)=\\max(\\alpha z,z) $$\n",
    "\n",
    "后续的，还有随机泄露ReLU（RReLU），在训练过程中在给定范围内随机选择 $\\alpha $\n",
    "\n",
    "有参数化泄露ReLU（PReLU）， $\\alpha $将在训练期间学习而不作为超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一种激活函数，指数线性单位（Exponential Linear Unit，ELU）\n",
    "\n",
    "$$\n",
    "ELU(z)=\n",
    "\\begin{cases}\n",
    "\n",
    "\\alpha(e^z - 1)\\quad if\\quad z<0\\\\\n",
    "z\\quad if\\quad z\\geq 0\n",
    "\n",
    "\n",
    "\\end{cases}\n",
    "$$\n",
    "如果 $\\alpha=1 $则函数所有位置都是平滑的，有助于梯度加速下降\n",
    "\n",
    "ELU的主要缺点时它的计算比ReLU及变体要慢\n",
    "\n",
    "SELU 可以使得网路是自归一化的，但是有条件p298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 1.2936 - accuracy: 0.6064 - val_loss: 0.8771 - val_accuracy: 0.7152\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.7899 - accuracy: 0.7424 - val_loss: 0.7055 - val_accuracy: 0.7706\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.6771 - accuracy: 0.7810 - val_loss: 0.6373 - val_accuracy: 0.7912\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.6178 - accuracy: 0.7972 - val_loss: 0.5827 - val_accuracy: 0.8092\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.5797 - accuracy: 0.8079 - val_loss: 0.5511 - val_accuracy: 0.8148\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.5524 - accuracy: 0.8154 - val_loss: 0.5280 - val_accuracy: 0.8264\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.5317 - accuracy: 0.8205 - val_loss: 0.5097 - val_accuracy: 0.8318\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.5158 - accuracy: 0.8257 - val_loss: 0.5019 - val_accuracy: 0.8328\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.5029 - accuracy: 0.8288 - val_loss: 0.4848 - val_accuracy: 0.8396\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4919 - accuracy: 0.8313 - val_loss: 0.4775 - val_accuracy: 0.8432\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Batch Normaliztion批量归一化\n",
    "  \n",
    "    更换激活函数成功缓解了训练一开始时的梯度爆炸、消失，而无法保证训练一段时间后的旧疾复发\n",
    "\n",
    "    批量归一化通过让模型学习各层输入的最佳缩放和均值，从而缓解训练一段时间后的梯度爆炸、消失\n",
    "* 大致方法\n",
    "\n",
    "    在隐藏层的激活函数前，通过该层输入的均值和标准差的 **移动平均值**来估计训练期间的最终统计信息\n",
    "\n",
    "    1. 通过反向传播想学习输出缩放向量$\\gamma $和输出偏移向量$\\beta $\n",
    "    2. 学习使用指数移动平均值估计的最终的输入均值向量$\\mu $和最终输入标准差向量$\\sigma $\n",
    "* 具体实现\n",
    "\n",
    "    使用API实现非常简单直观，只需在每个隐藏层的激活函数之前或之后添加一个批量归一化的层\n",
    "\n",
    "    但是BN论文的作者主张在激活函数之前添加批量归一化层\n",
    "\n",
    "    首先是在激活后使用BN，随后是在激活前使用BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # 可选地在模型的第一层后添加一个BN\n",
    "    keras.layers.BatchNormalization(), \n",
    "    # 在激活函数后添加BN \n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个BN层的每个输入添加了四个参数\n",
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在试试在激活之前使用BN层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # 可选地在模型的第一层之后添加一个BN层\n",
    "    keras.layers.BatchNormalization(), \n",
    "    # 在激活函数之前使用BN， \n",
    "    # 在BN层之前的层不需要偏置项，因为BN自带偏置项\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向传播期间裁剪梯度，让它们永远不会超过某个阈值，是另一个缓解梯度爆炸的流行技术\n",
    "\n",
    "keras中仅仅需要设置clipvalue或clipnorm参数，即可实现梯度裁剪\n",
    "\n",
    "该优化器会将梯度向量的每个分量都蔡建伟-1.0到1.0之间的值，即将所有损失的偏导数限制在这个范围之间，阈值是可以调整的超参数\n",
    "\n",
    "应当通过设置clipnorm而不是clipvalue按照范数裁剪，确保梯度裁剪不会改变梯度的方向（按值裁剪、按范数裁剪）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import optimize\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cd7ecbf46b3498f952f614292bd9b98de37d868a382baadab1fa3f5188ed6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
