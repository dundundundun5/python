{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用特定初始化是让反向传播时，传播前和传播后，某层方差保持不变，从而缓解梯度爆炸和梯度消失的问题\n",
    "\n",
    "普通的sigmoid函数的平均值为0.5并非0，导致神经元输出的方差将大于输入的方差\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glorot 和 He 初始化\n",
    "[name for name in dir(keras.initializers) if not name.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|初始化|激活函数|$\\sigma^2$|\n",
    "|-|-|-|\n",
    "|Glorot|None,tanh,logistic,softmax|$\\frac{1}{fan_{avg}}$|\n",
    "|He|ReLU和变体|$\\frac{2}{fan_{in}}$|\n",
    "|LeCun|SELU|$\\frac{1}{fan_{in}}$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x2c2f0eafa60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x2c2b8fe36a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果使用均匀分布但基于fan_avg而不是fan_in\n",
    "# 进行He初始化，则可以使用Variance_Scaling初始化\n",
    "init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg'\n",
    "                                , distribution='uniform')\n",
    "        \n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比于ReLU，leaky ReLU能够防止加权和为负数时，神经元只会输出0导致神经元死亡\n",
    "$$ReLU(z)=\\max(0,z) $$\n",
    "\n",
    "$$leakyReLU(z)=\\max(\\alpha z,z) $$\n",
    "\n",
    "后续的，还有随机泄露ReLU（RReLU），在训练过程中在给定范围内随机选择 $\\alpha $\n",
    "\n",
    "有参数化泄露ReLU（PReLU）， $\\alpha $将在训练期间学习而不作为超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一种激活函数，指数线性单位（Exponential Linear Unit，ELU）\n",
    "\n",
    "$$\n",
    "ELU(z)=\n",
    "\\begin{cases}\n",
    "\n",
    "\\alpha(e^z - 1)\\quad if\\quad z<0\\\\\n",
    "z\\quad if\\quad z\\geq 0\n",
    "\n",
    "\n",
    "\\end{cases}\n",
    "$$\n",
    "如果 $\\alpha=1 $则函数所有位置都是平滑的，有助于梯度加速下降\n",
    "\n",
    "ELU的主要缺点时它的计算比ReLU及变体要慢\n",
    "\n",
    "SELU 可以使得网路是自归一化的，但是有条件p298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 11s 6ms/step - loss: 1.2897 - accuracy: 0.6043 - val_loss: 0.8785 - val_accuracy: 0.7236\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Batch Normaliztion批量归一化\n",
    "  \n",
    "    更换激活函数成功缓解了训练一开始时的梯度爆炸、消失，而无法保证训练一段时间后的旧疾复发\n",
    "\n",
    "    批量归一化通过让模型学习各层输入的最佳缩放和均值，从而缓解训练一段时间后的梯度爆炸、消失\n",
    "* 大致方法\n",
    "\n",
    "    在隐藏层的激活函数前，通过该层输入的均值和标准差的 **移动平均值**来估计训练期间的最终统计信息\n",
    "\n",
    "    1. 通过反向传播想学习输出缩放向量$\\gamma $和输出偏移向量$\\beta $\n",
    "    2. 学习使用指数移动平均值估计的最终的输入均值向量$\\mu $和最终输入标准差向量$\\sigma $\n",
    "* 具体实现\n",
    "\n",
    "    使用API实现非常简单直观，只需在每个隐藏层的激活函数之前或之后添加一个批量归一化的层\n",
    "\n",
    "    但是BN论文的作者主张在激活函数之前添加批量归一化层\n",
    "\n",
    "    首先是在激活后使用BN，随后是在激活前使用BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # 可选地在模型的第一层后添加一个BN\n",
    "    keras.layers.BatchNormalization(), \n",
    "    # 在激活函数后添加BN \n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个BN层的每个输入添加了四个参数\n",
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 25s 14ms/step - loss: 0.8254 - accuracy: 0.7207 - val_loss: 0.5540 - val_accuracy: 0.8104\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                    validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在试试在激活之前使用BN层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # 可选地在模型的第一层之后添加一个BN层\n",
    "    keras.layers.BatchNormalization(), \n",
    "    # 在激活函数之前使用BN， \n",
    "    # 在BN层之前的层不需要偏置项，因为BN自带偏置项\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 24s 14ms/step - loss: 1.0333 - accuracy: 0.6750 - val_loss: 0.6803 - val_accuracy: 0.7852\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向传播期间裁剪梯度，让它们永远不会超过某个阈值，是另一个缓解梯度爆炸的流行技术\n",
    "\n",
    "keras中仅仅需要设置clipvalue或clipnorm参数，即可实现梯度裁剪\n",
    "\n",
    "该优化器会将梯度向量的每个分量都蔡建伟-1.0到1.0之间的值，即将所有损失的偏导数限制在这个范围之间，阈值是可以调整的超参数\n",
    "\n",
    "应当通过设置clipnorm而不是clipvalue按照范数裁剪，确保梯度裁剪不会改变梯度的方向（按值裁剪、按范数裁剪）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import optimize\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用keras进行迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割fashion Mnist数据集，返回两个训练集\n",
    "# A训练集 X-标签不是凉鞋且不是衬衫的所有图片，y-X的标签列（多分类器）\n",
    "# B训练集 X-标签是凉鞋或衬衫的所有图片，y-X的标签是否为衬衫的标签列（二元分类器）\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "# 按类别分离Fashion Mnist的三大集合\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3, 4, 5,\n",
       "       1, 3, 4, 2, 0, 6, 7, 1], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_A[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_B[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组件模型A，编译并训练\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 14s 9ms/step - loss: 0.5515 - accuracy: 0.8223 - val_loss: 0.3899 - val_accuracy: 0.8637\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 14s 10ms/step - loss: 0.3530 - accuracy: 0.8794 - val_loss: 0.3299 - val_accuracy: 0.8847\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 14s 10ms/step - loss: 0.3148 - accuracy: 0.8909 - val_loss: 0.3002 - val_accuracy: 0.8946\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 13s 9ms/step - loss: 0.2950 - accuracy: 0.8981 - val_loss: 0.2853 - val_accuracy: 0.8996\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 13s 9ms/step - loss: 0.2819 - accuracy: 0.9025 - val_loss: 0.2768 - val_accuracy: 0.9023\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 13s 9ms/step - loss: 0.2723 - accuracy: 0.9070 - val_loss: 0.2727 - val_accuracy: 0.9051\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 13s 9ms/step - loss: 0.2642 - accuracy: 0.9090 - val_loss: 0.2685 - val_accuracy: 0.9056\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 14s 10ms/step - loss: 0.2577 - accuracy: 0.9123 - val_loss: 0.2582 - val_accuracy: 0.9121\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 13s 10ms/step - loss: 0.2523 - accuracy: 0.9138 - val_loss: 0.2553 - val_accuracy: 0.9133\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 13s 10ms/step - loss: 0.2473 - accuracy: 0.9150 - val_loss: 0.2543 - val_accuracy: 0.9116\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 13s 10ms/step - loss: 0.2431 - accuracy: 0.9161 - val_loss: 0.2484 - val_accuracy: 0.9153\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 13s 9ms/step - loss: 0.2389 - accuracy: 0.9177 - val_loss: 0.2482 - val_accuracy: 0.9150\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 14s 10ms/step - loss: 0.2354 - accuracy: 0.9192 - val_loss: 0.2428 - val_accuracy: 0.9148\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 13s 9ms/step - loss: 0.2322 - accuracy: 0.9210 - val_loss: 0.2403 - val_accuracy: 0.9145\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 13s 10ms/step - loss: 0.2293 - accuracy: 0.9212 - val_loss: 0.2448 - val_accuracy: 0.9153\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 13s 9ms/step - loss: 0.2263 - accuracy: 0.9227 - val_loss: 0.2372 - val_accuracy: 0.9183\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 12s 9ms/step - loss: 0.2238 - accuracy: 0.9233 - val_loss: 0.2406 - val_accuracy: 0.9168\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 13s 10ms/step - loss: 0.2208 - accuracy: 0.9243 - val_loss: 0.2442 - val_accuracy: 0.9131\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 12s 9ms/step - loss: 0.2186 - accuracy: 0.9250 - val_loss: 0.2346 - val_accuracy: 0.9185\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 13s 9ms/step - loss: 0.2163 - accuracy: 0.9259 - val_loss: 0.2324 - val_accuracy: 0.9193\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "# 参考模型A的架构并训练一个新模型 \n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.6294 - accuracy: 0.6850 - val_loss: 0.5325 - val_accuracy: 0.7748\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.4365 - accuracy: 0.8400 - val_loss: 0.4283 - val_accuracy: 0.8377\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.3464 - accuracy: 0.8850 - val_loss: 0.3589 - val_accuracy: 0.8986\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.2871 - accuracy: 0.9300 - val_loss: 0.3082 - val_accuracy: 0.9168\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.2425 - accuracy: 0.9450 - val_loss: 0.2726 - val_accuracy: 0.9320\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.2110 - accuracy: 0.9600 - val_loss: 0.2439 - val_accuracy: 0.9442\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.1866 - accuracy: 0.9700 - val_loss: 0.2241 - val_accuracy: 0.9493\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.1688 - accuracy: 0.9700 - val_loss: 0.2064 - val_accuracy: 0.9544\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.1533 - accuracy: 0.9700 - val_loss: 0.1866 - val_accuracy: 0.9615\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.1383 - accuracy: 0.9800 - val_loss: 0.1748 - val_accuracy: 0.9655\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.1273 - accuracy: 0.9850 - val_loss: 0.1642 - val_accuracy: 0.9696\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.1182 - accuracy: 0.9850 - val_loss: 0.1549 - val_accuracy: 0.9696\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.1097 - accuracy: 0.9850 - val_loss: 0.1461 - val_accuracy: 0.9726\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.1022 - accuracy: 0.9850 - val_loss: 0.1386 - val_accuracy: 0.9736\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0961 - accuracy: 0.9850 - val_loss: 0.1319 - val_accuracy: 0.9746\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0907 - accuracy: 0.9900 - val_loss: 0.1268 - val_accuracy: 0.9746\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0860 - accuracy: 0.9900 - val_loss: 0.1221 - val_accuracy: 0.9746\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0818 - accuracy: 0.9900 - val_loss: 0.1175 - val_accuracy: 0.9757\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0778 - accuracy: 0.9900 - val_loss: 0.1135 - val_accuracy: 0.9777\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0742 - accuracy: 0.9950 - val_loss: 0.1100 - val_accuracy: 0.9777\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_7 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务B与任务A非常相似，上面的模型B通过模仿模型A的架构来完成训练\n",
    "\n",
    "现在也可以通过迁移学习来完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x2c3b841f400>,\n",
       " <keras.layers.core.dense.Dense at 0x2c3bf12ada0>,\n",
       " <keras.layers.core.dense.Dense at 0x2c3ba5adc30>,\n",
       " <keras.layers.core.dense.Dense at 0x2c3bf129690>,\n",
       " <keras.layers.core.dense.Dense at 0x2c2e91f85e0>,\n",
       " <keras.layers.core.dense.Dense at 0x2c3b841c760>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "# 获取除了输出层之前的所有层架构及其训练好的权重\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_A.layers[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在model_A和model_B_on_A共享一些层，对后者的训练也会影响前者的层权重\n",
    "\n",
    "如果想要分离控制二者，则必须使用api，clone_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于新的输出层是随机初始化的，可能存在较大的错误梯度，可能会破坏重用模型A的现成权重\n",
    "\n",
    "* 冻结训练\n",
    "\n",
    "    为了避免以上情况，就是在训练的前几个伦茨冻结重用的层，给新层一些时间来学习合理的权重\n",
    "* 代码\n",
    "\n",
    "    将每一层的可训练属性设置为False并编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 冻结重用的前几层（此处是除了是输出层以外的所有层）\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy',\n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.6020 - accuracy: 0.7000 - val_loss: 0.5433 - val_accuracy: 0.7525\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.5722 - accuracy: 0.7300 - val_loss: 0.5162 - val_accuracy: 0.7708\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.5430 - accuracy: 0.7400 - val_loss: 0.4927 - val_accuracy: 0.7819\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.5174 - accuracy: 0.7500 - val_loss: 0.4698 - val_accuracy: 0.7972\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.4317 - accuracy: 0.8050 - val_loss: 0.3198 - val_accuracy: 0.8742\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.2911 - accuracy: 0.8950 - val_loss: 0.2389 - val_accuracy: 0.9168\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 0.2141 - accuracy: 0.9400 - val_loss: 0.1927 - val_accuracy: 0.9422\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.1689 - accuracy: 0.9550 - val_loss: 0.1599 - val_accuracy: 0.9533\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.1360 - accuracy: 0.9650 - val_loss: 0.1367 - val_accuracy: 0.9655\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.1121 - accuracy: 0.9900 - val_loss: 0.1205 - val_accuracy: 0.9716\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0952 - accuracy: 0.9950 - val_loss: 0.1088 - val_accuracy: 0.9736\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0832 - accuracy: 1.0000 - val_loss: 0.0996 - val_accuracy: 0.9757\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0736 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9797\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 0.0852 - val_accuracy: 0.9797\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0594 - accuracy: 1.0000 - val_loss: 0.0808 - val_accuracy: 0.9817\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9807\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0507 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9817\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0474 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 0.9838\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0446 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9848\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                            validation_data=(X_valid_B, y_valid_B))\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                            validation_data=(X_valid_B, y_valid_B))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0993 - accuracy: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09932302683591843, 0.9865000247955322]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0606 - accuracy: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.060557588934898376, 0.9915000200271606]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.588235294117651"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 - 98.65) / (100 - 99.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试的精度高达98.58%, 错误率也降了1.59倍\n",
    "\n",
    "这么做实际上是“折磨数据直到信服位置”\n",
    "\n",
    "当论文看起来过于优秀时，你应该要怀疑：也许这个浮华的新技术实际上并没有多大的帮助（事实上，它甚至可能降低性能），但作者尝试了许多变体，仅报告了最好的结果（这可能是由于运气所致），而没有提及他们在途中遇到了多少次失败。\n",
    "\n",
    "在大多数情况下，这根本不是恶意的，但这是造成如此多科学结果永远无法复现的部分原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，迁移学习在小型密集网络上不能很好地工作，可能因为小型网络学习的模式很少，密集网络学习的是非常特定的模式，在其他任务中不是很有用\n",
    "\n",
    "迁移学习最适合深度卷积神经网络，该神经网络倾向于学习更为通用的特征检测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更快的优化器\n",
    "\n",
    "<font color='red'>公式中所有更新的变量均为向量</font>\n",
    "* 动量优化\n",
    "\n",
    "    动量优化非常关心先前的梯度（加速度概念），每次迭代都会从动量向量m中减去局部梯度，并通过添加该动量向量来更新权重\n",
    "\n",
    "    为了模拟摩擦机制并防止动量变得过大，引入超参数$\\beta $，另外$\\eta $还是学习率\n",
    "\n",
    "    $$m\\leftarrow\\beta m-\\eta\\nabla_{\\theta}J(\\theta) $$\n",
    "    $$\\theta\\leftarrow\\theta+m $$\n",
    "* Nesterov加速梯度\n",
    "\n",
    "    又称NNesterov动量优化，是动量优化的小变体，它不再局部位置$\\theta $，而在$\\theta+\\beta m处沿动量方向稍微提前处测量成本函数的梯度 $\n",
    "\n",
    "    $$m\\leftarrow\\beta m-\\eta\\nabla_{\\theta}J(\\theta+\\beta m) $$\n",
    "    $$\\theta\\leftarrow\\theta+m $$\n",
    "\n",
    "    这种小的调整有效是因为通常动量向量会指向正确的方向，因此使用在该方向上测得的更远的梯度而不是原始位置上的梯度会稍微准确一些\n",
    "\n",
    "* AdaGrad\n",
    "\n",
    "    通过沿着最陡峭的维度按比例缩小梯度向量\n",
    "\n",
    "    此处是逐个元素相乘，相除\n",
    "    $$s\\leftarrow s+ \\nabla_{\\theta}J(\\theta)\\times\\nabla_{\\theta}J(\\theta)$$\n",
    "    $$\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}J(\\theta) / \\sqrt{s+\\epsilon} $$\n",
    "\n",
    "    1. 将梯度的平方累计到向量s中\n",
    "    2. 梯度向量按比例因子$\\sqrt{s+\\epsilon} $缩小了，其中$\\epsilon是平滑项 $\n",
    "    \n",
    "    *该算法会降低学习率*，但是对于陡峭的维度，它的执行速度比对缓慢下降的维度的执行速度要快。这称为自适应学习率。好处是算法几乎不需要调整学习率超参数$\\eta$\n",
    "* RMSProp\n",
    "    \n",
    "    AdaGrad有可能下降太快，从而永远不能收敛到全局最优解。RMSProp解决了这个问题。\n",
    "\n",
    "    其通过只是累加最近迭代中的梯度（而不是自训练依赖的所有梯度）来解决这个问题\n",
    "\n",
    "    $$s\\leftarrow \\beta s+ (1-\\beta)\\nabla_{\\theta}J(\\theta)\\times\\nabla_{\\theta}J(\\theta)$$\n",
    "    $$\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}J(\\theta) / \\sqrt{s+\\epsilon} $$\n",
    "\n",
    "    衰减率$\\beta$是一个新的超参数，设置为0.9,通过施加衰减率相关的加权和，强调了最近迭代的梯度，弱化了梯度平方的权重从而避免像AdaGrad那样下降太快\n",
    "\n",
    "* Adam\n",
    "\n",
    "    结合了动量优化和RMSProp的思想，代表着自适应矩估计,(逐个元素相乘、逐个元素相除)\n",
    "    \n",
    "    1. $$m\\leftarrow\\beta_1 m-(1-\\beta_1)\\nabla_{\\theta}J(\\theta) $$ \n",
    "    \n",
    "    2. $$s\\leftarrow \\beta_2s+ (1-\\beta_2)\\nabla_{\\theta}J(\\theta)\\times\\nabla_{\\theta}J(\\theta)$$\n",
    "    3. $$\\hat m\\leftarrow\\frac{m}{1-\\beta^t_1} $$\n",
    "    4. $$\\hat s\\leftarrow\\frac{s}{1-\\beta_2^t} $$\n",
    "    5. $$\\theta\\leftarrow\\theta-\\eta\\hat m / \\sqrt{s+\\epsilon} $$\n",
    "\n",
    "    1、2、5与动量优化和RMSProp非常相似\n",
    "    3、4由于m和s初始化为0，因此在训练开始时它们会偏向0，这两个个步骤有助于在训练开始时提高m和s\n",
    "\n",
    "    Adam也是一种自适应学习率算法，因此对学习率超参数$\\eta$需要较少的调整\n",
    "\n",
    "* Nadam\n",
    "  \n",
    "    是在Adam优化上加上了Nesterov技巧，因此其收敛速度通常比Adam快\n",
    "\n",
    "**优化器比较P317**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练稀疏模型**\n",
    "\n",
    "在训练时使用强l1正规化，迫使优化器产生尽可能多的为0的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现动量优化, 0.9是一个动量可参考的超参值\n",
    "from re import T\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
    "# 实现Nesterov动量优化\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9, nesterov=True)\n",
    "# 不应该用AdaGrad\n",
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "# 使用RMSProp\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "# 使用Adam beta_1和beta_2的参数是实用的，并非随意给出\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "# 使用Nadam\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率调度P318\n",
    "* 幂调度\n",
    "\n",
    "    $$\\eta(t)=\\frac{\\eta_0}{(1+\\frac{t}{s})^c} $$\n",
    "    $$\\eta_0是初始学习率 \\quad t是迭代次数$$\n",
    "    $$s是每s步学习率下降一次，第i次下降将下降到\\frac{\\eta_0}{i} $$\n",
    "* 指数调度\n",
    "    $$\\eta(t)= \\eta_0 0.1^{\\frac{t}{s}}$$\n",
    "* 分段恒定调度\n",
    "* 性能调度\n",
    "* 1周期调度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用幂调度 decay参数是s的倒数\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cd7ecbf46b3498f952f614292bd9b98de37d868a382baadab1fa3f5188ed6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
