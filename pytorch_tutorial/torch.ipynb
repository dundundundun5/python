{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.707095  [    0/ 6954]\n",
      "loss: 0.701680  [ 3200/ 6954]\n",
      "loss: 0.685630  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.689319 \n",
      "\n",
      "loss: 0.695861  [    0/ 6954]\n",
      "loss: 0.698382  [ 3200/ 6954]\n",
      "loss: 0.689580  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.683768 \n",
      "\n",
      "loss: 0.695649  [    0/ 6954]\n",
      "loss: 0.676761  [ 3200/ 6954]\n",
      "loss: 0.666592  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.677925 \n",
      "\n",
      "loss: 0.683072  [    0/ 6954]\n",
      "loss: 0.666905  [ 3200/ 6954]\n",
      "loss: 0.696212  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.671709 \n",
      "\n",
      "loss: 0.666471  [    0/ 6954]\n",
      "loss: 0.662152  [ 3200/ 6954]\n",
      "loss: 0.657031  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.664348 \n",
      "\n",
      "loss: 0.672369  [    0/ 6954]\n",
      "loss: 0.673547  [ 3200/ 6954]\n",
      "loss: 0.660300  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.657152 \n",
      "\n",
      "loss: 0.659226  [    0/ 6954]\n",
      "loss: 0.650813  [ 3200/ 6954]\n",
      "loss: 0.620361  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.647893 \n",
      "\n",
      "loss: 0.670362  [    0/ 6954]\n",
      "loss: 0.630344  [ 3200/ 6954]\n",
      "loss: 0.624040  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.638594 \n",
      "\n",
      "loss: 0.670015  [    0/ 6954]\n",
      "loss: 0.611718  [ 3200/ 6954]\n",
      "loss: 0.617233  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.628935 \n",
      "\n",
      "loss: 0.639305  [    0/ 6954]\n",
      "loss: 0.601815  [ 3200/ 6954]\n",
      "loss: 0.608203  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.619377 \n",
      "\n",
      "loss: 0.588408  [    0/ 6954]\n",
      "loss: 0.582669  [ 3200/ 6954]\n",
      "loss: 0.620473  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.609795 \n",
      "\n",
      "loss: 0.566221  [    0/ 6954]\n",
      "loss: 0.606410  [ 3200/ 6954]\n",
      "loss: 0.596271  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.602999 \n",
      "\n",
      "loss: 0.555061  [    0/ 6954]\n",
      "loss: 0.674052  [ 3200/ 6954]\n",
      "loss: 0.573621  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.592927 \n",
      "\n",
      "loss: 0.546203  [    0/ 6954]\n",
      "loss: 0.647759  [ 3200/ 6954]\n",
      "loss: 0.670901  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.586344 \n",
      "\n",
      "loss: 0.577594  [    0/ 6954]\n",
      "loss: 0.607186  [ 3200/ 6954]\n",
      "loss: 0.564771  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.579451 \n",
      "\n",
      "loss: 0.586447  [    0/ 6954]\n",
      "loss: 0.591553  [ 3200/ 6954]\n",
      "loss: 0.649624  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.574543 \n",
      "\n",
      "loss: 0.533098  [    0/ 6954]\n",
      "loss: 0.573818  [ 3200/ 6954]\n",
      "loss: 0.582119  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.570794 \n",
      "\n",
      "loss: 0.580732  [    0/ 6954]\n",
      "loss: 0.551901  [ 3200/ 6954]\n",
      "loss: 0.559095  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.566238 \n",
      "\n",
      "loss: 0.500565  [    0/ 6954]\n",
      "loss: 0.660372  [ 3200/ 6954]\n",
      "loss: 0.620067  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.561515 \n",
      "\n",
      "loss: 0.617363  [    0/ 6954]\n",
      "loss: 0.591190  [ 3200/ 6954]\n",
      "loss: 0.566329  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.561886 \n",
      "\n",
      "loss: 0.594377  [    0/ 6954]\n",
      "loss: 0.454070  [ 3200/ 6954]\n",
      "loss: 0.645213  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.559887 \n",
      "\n",
      "loss: 0.578169  [    0/ 6954]\n",
      "loss: 0.686287  [ 3200/ 6954]\n",
      "loss: 0.607830  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.556928 \n",
      "\n",
      "loss: 0.642921  [    0/ 6954]\n",
      "loss: 0.594216  [ 3200/ 6954]\n",
      "loss: 0.537014  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.555271 \n",
      "\n",
      "loss: 0.624543  [    0/ 6954]\n",
      "loss: 0.642413  [ 3200/ 6954]\n",
      "loss: 0.548624  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.551406 \n",
      "\n",
      "loss: 0.627130  [    0/ 6954]\n",
      "loss: 0.456446  [ 3200/ 6954]\n",
      "loss: 0.682708  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.552572 \n",
      "\n",
      "loss: 0.680759  [    0/ 6954]\n",
      "loss: 0.741007  [ 3200/ 6954]\n",
      "loss: 0.640311  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.552672 \n",
      "\n",
      "loss: 0.473049  [    0/ 6954]\n",
      "loss: 0.570221  [ 3200/ 6954]\n",
      "loss: 0.438431  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.553311 \n",
      "\n",
      "loss: 0.593989  [    0/ 6954]\n",
      "loss: 0.662541  [ 3200/ 6954]\n",
      "loss: 0.651391  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.551094 \n",
      "\n",
      "loss: 0.634181  [    0/ 6954]\n",
      "loss: 0.586103  [ 3200/ 6954]\n",
      "loss: 0.589277  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.548292 \n",
      "\n",
      "loss: 0.466171  [    0/ 6954]\n",
      "loss: 0.516700  [ 3200/ 6954]\n",
      "loss: 0.643975  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.553031 \n",
      "\n",
      "loss: 0.695325  [    0/ 6954]\n",
      "loss: 0.500175  [ 3200/ 6954]\n",
      "loss: 0.721404  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.549967 \n",
      "\n",
      "loss: 0.600478  [    0/ 6954]\n",
      "loss: 0.496682  [ 3200/ 6954]\n",
      "loss: 0.537727  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.548938 \n",
      "\n",
      "loss: 0.696226  [    0/ 6954]\n",
      "loss: 0.525604  [ 3200/ 6954]\n",
      "loss: 0.498151  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.550928 \n",
      "\n",
      "loss: 0.428687  [    0/ 6954]\n",
      "loss: 0.661949  [ 3200/ 6954]\n",
      "loss: 0.421679  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.548384 \n",
      "\n",
      "loss: 0.660496  [    0/ 6954]\n",
      "loss: 0.612540  [ 3200/ 6954]\n",
      "loss: 0.533586  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.547999 \n",
      "\n",
      "loss: 0.498489  [    0/ 6954]\n",
      "loss: 0.587893  [ 3200/ 6954]\n",
      "loss: 0.478584  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.547674 \n",
      "\n",
      "loss: 0.523622  [    0/ 6954]\n",
      "loss: 0.431947  [ 3200/ 6954]\n",
      "loss: 0.461610  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.548884 \n",
      "\n",
      "loss: 0.426795  [    0/ 6954]\n",
      "loss: 0.554089  [ 3200/ 6954]\n",
      "loss: 0.546217  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.548328 \n",
      "\n",
      "loss: 0.522973  [    0/ 6954]\n",
      "loss: 0.435099  [ 3200/ 6954]\n",
      "loss: 0.600909  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.547058 \n",
      "\n",
      "loss: 0.495502  [    0/ 6954]\n",
      "loss: 0.631759  [ 3200/ 6954]\n",
      "loss: 0.561408  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.546076 \n",
      "\n",
      "loss: 0.572918  [    0/ 6954]\n",
      "loss: 0.597337  [ 3200/ 6954]\n",
      "loss: 0.504889  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.548793 \n",
      "\n",
      "loss: 0.573080  [    0/ 6954]\n",
      "loss: 0.543582  [ 3200/ 6954]\n",
      "loss: 0.560426  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.548760 \n",
      "\n",
      "loss: 0.695927  [    0/ 6954]\n",
      "loss: 0.535592  [ 3200/ 6954]\n",
      "loss: 0.645092  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.547607 \n",
      "\n",
      "loss: 0.432942  [    0/ 6954]\n",
      "loss: 0.635843  [ 3200/ 6954]\n",
      "loss: 0.649774  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.546261 \n",
      "\n",
      "loss: 0.564153  [    0/ 6954]\n",
      "loss: 0.556672  [ 3200/ 6954]\n",
      "loss: 0.566599  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.545774 \n",
      "\n",
      "loss: 0.684182  [    0/ 6954]\n",
      "loss: 0.466752  [ 3200/ 6954]\n",
      "loss: 0.479350  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.546237 \n",
      "\n",
      "loss: 0.571687  [    0/ 6954]\n",
      "loss: 0.437365  [ 3200/ 6954]\n",
      "loss: 0.465632  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.543962 \n",
      "\n",
      "loss: 0.624448  [    0/ 6954]\n",
      "loss: 0.477486  [ 3200/ 6954]\n",
      "loss: 0.632716  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.543435 \n",
      "\n",
      "loss: 0.500297  [    0/ 6954]\n",
      "loss: 0.569329  [ 3200/ 6954]\n",
      "loss: 0.570194  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.547401 \n",
      "\n",
      "loss: 0.634457  [    0/ 6954]\n",
      "loss: 0.462587  [ 3200/ 6954]\n",
      "loss: 0.641485  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.544959 \n",
      "\n",
      "loss: 0.576830  [    0/ 6954]\n",
      "loss: 0.531940  [ 3200/ 6954]\n",
      "loss: 0.641214  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.546889 \n",
      "\n",
      "loss: 0.476382  [    0/ 6954]\n",
      "loss: 0.512258  [ 3200/ 6954]\n",
      "loss: 0.445002  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.543769 \n",
      "\n",
      "loss: 0.469996  [    0/ 6954]\n",
      "loss: 0.459075  [ 3200/ 6954]\n",
      "loss: 0.405305  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.546593 \n",
      "\n",
      "loss: 0.530424  [    0/ 6954]\n",
      "loss: 0.384479  [ 3200/ 6954]\n",
      "loss: 0.446192  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.544425 \n",
      "\n",
      "loss: 0.518860  [    0/ 6954]\n",
      "loss: 0.510188  [ 3200/ 6954]\n",
      "loss: 0.547656  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.551621 \n",
      "\n",
      "loss: 0.573184  [    0/ 6954]\n",
      "loss: 0.597720  [ 3200/ 6954]\n",
      "loss: 0.492017  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.546869 \n",
      "\n",
      "loss: 0.604477  [    0/ 6954]\n",
      "loss: 0.554697  [ 3200/ 6954]\n",
      "loss: 0.485332  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.546676 \n",
      "\n",
      "loss: 0.488694  [    0/ 6954]\n",
      "loss: 0.639745  [ 3200/ 6954]\n",
      "loss: 0.590015  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.546949 \n",
      "\n",
      "loss: 0.446820  [    0/ 6954]\n",
      "loss: 0.479497  [ 3200/ 6954]\n",
      "loss: 0.294617  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.543966 \n",
      "\n",
      "loss: 0.329832  [    0/ 6954]\n",
      "loss: 0.550441  [ 3200/ 6954]\n",
      "loss: 0.443034  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.547037 \n",
      "\n",
      "loss: 0.559559  [    0/ 6954]\n",
      "loss: 0.487971  [ 3200/ 6954]\n",
      "loss: 0.499620  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.545094 \n",
      "\n",
      "loss: 0.462563  [    0/ 6954]\n",
      "loss: 0.604910  [ 3200/ 6954]\n",
      "loss: 0.554056  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.544891 \n",
      "\n",
      "loss: 0.468683  [    0/ 6954]\n",
      "loss: 0.683868  [ 3200/ 6954]\n",
      "loss: 0.735289  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.543983 \n",
      "\n",
      "loss: 0.597699  [    0/ 6954]\n",
      "loss: 0.535635  [ 3200/ 6954]\n",
      "loss: 0.427764  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.546623 \n",
      "\n",
      "loss: 0.363079  [    0/ 6954]\n",
      "loss: 0.543837  [ 3200/ 6954]\n",
      "loss: 0.556817  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.548512 \n",
      "\n",
      "loss: 0.578585  [    0/ 6954]\n",
      "loss: 0.440786  [ 3200/ 6954]\n",
      "loss: 0.587827  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.547670 \n",
      "\n",
      "loss: 0.502389  [    0/ 6954]\n",
      "loss: 0.649542  [ 3200/ 6954]\n",
      "loss: 0.532925  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.545152 \n",
      "\n",
      "loss: 0.534241  [    0/ 6954]\n",
      "loss: 0.479747  [ 3200/ 6954]\n",
      "loss: 0.660239  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.544263 \n",
      "\n",
      "loss: 0.595321  [    0/ 6954]\n",
      "loss: 0.577757  [ 3200/ 6954]\n",
      "loss: 0.535822  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.543892 \n",
      "\n",
      "loss: 0.563610  [    0/ 6954]\n",
      "loss: 0.474521  [ 3200/ 6954]\n",
      "loss: 0.604475  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.543835 \n",
      "\n",
      "loss: 0.558784  [    0/ 6954]\n",
      "loss: 0.625229  [ 3200/ 6954]\n",
      "loss: 0.631467  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.544014 \n",
      "\n",
      "loss: 0.423110  [    0/ 6954]\n",
      "loss: 0.614919  [ 3200/ 6954]\n",
      "loss: 0.518991  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.545438 \n",
      "\n",
      "loss: 0.685430  [    0/ 6954]\n",
      "loss: 0.617945  [ 3200/ 6954]\n",
      "loss: 0.522519  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.544448 \n",
      "\n",
      "loss: 0.451331  [    0/ 6954]\n",
      "loss: 0.533097  [ 3200/ 6954]\n",
      "loss: 0.533709  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.540719 \n",
      "\n",
      "loss: 0.570037  [    0/ 6954]\n",
      "loss: 0.538350  [ 3200/ 6954]\n",
      "loss: 0.637543  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.543528 \n",
      "\n",
      "loss: 0.613565  [    0/ 6954]\n",
      "loss: 0.560465  [ 3200/ 6954]\n",
      "loss: 0.524745  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.544088 \n",
      "\n",
      "loss: 0.411033  [    0/ 6954]\n",
      "loss: 0.460954  [ 3200/ 6954]\n",
      "loss: 0.454693  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.546614 \n",
      "\n",
      "loss: 0.587320  [    0/ 6954]\n",
      "loss: 0.592776  [ 3200/ 6954]\n",
      "loss: 0.614956  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.543726 \n",
      "\n",
      "loss: 0.524583  [    0/ 6954]\n",
      "loss: 0.605182  [ 3200/ 6954]\n",
      "loss: 0.405467  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.545599 \n",
      "\n",
      "loss: 0.524621  [    0/ 6954]\n",
      "loss: 0.584344  [ 3200/ 6954]\n",
      "loss: 0.413560  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.540928 \n",
      "\n",
      "loss: 0.616667  [    0/ 6954]\n",
      "loss: 0.672115  [ 3200/ 6954]\n",
      "loss: 0.647215  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.544801 \n",
      "\n",
      "loss: 0.585649  [    0/ 6954]\n",
      "loss: 0.573139  [ 3200/ 6954]\n",
      "loss: 0.475938  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.542572 \n",
      "\n",
      "loss: 0.455414  [    0/ 6954]\n",
      "loss: 0.540938  [ 3200/ 6954]\n",
      "loss: 0.550325  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.540749 \n",
      "\n",
      "loss: 0.500858  [    0/ 6954]\n",
      "loss: 0.467121  [ 3200/ 6954]\n",
      "loss: 0.512320  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.542456 \n",
      "\n",
      "loss: 0.469536  [    0/ 6954]\n",
      "loss: 0.609840  [ 3200/ 6954]\n",
      "loss: 0.563636  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.544394 \n",
      "\n",
      "loss: 0.652942  [    0/ 6954]\n",
      "loss: 0.524887  [ 3200/ 6954]\n",
      "loss: 0.606225  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.542898 \n",
      "\n",
      "loss: 0.431437  [    0/ 6954]\n",
      "loss: 0.421883  [ 3200/ 6954]\n",
      "loss: 0.445951  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.540142 \n",
      "\n",
      "loss: 0.625315  [    0/ 6954]\n",
      "loss: 0.562556  [ 3200/ 6954]\n",
      "loss: 0.476255  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.539692 \n",
      "\n",
      "loss: 0.542461  [    0/ 6954]\n",
      "loss: 0.477775  [ 3200/ 6954]\n",
      "loss: 0.539076  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.542407 \n",
      "\n",
      "loss: 0.494032  [    0/ 6954]\n",
      "loss: 0.479466  [ 3200/ 6954]\n",
      "loss: 0.440127  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.541734 \n",
      "\n",
      "loss: 0.500768  [    0/ 6954]\n",
      "loss: 0.576863  [ 3200/ 6954]\n",
      "loss: 0.577749  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.543436 \n",
      "\n",
      "loss: 0.471727  [    0/ 6954]\n",
      "loss: 0.454797  [ 3200/ 6954]\n",
      "loss: 0.605742  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.542289 \n",
      "\n",
      "loss: 0.489061  [    0/ 6954]\n",
      "loss: 0.555786  [ 3200/ 6954]\n",
      "loss: 0.527269  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.545481 \n",
      "\n",
      "loss: 0.611841  [    0/ 6954]\n",
      "loss: 0.592035  [ 3200/ 6954]\n",
      "loss: 0.527171  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.543970 \n",
      "\n",
      "loss: 0.553971  [    0/ 6954]\n",
      "loss: 0.649227  [ 3200/ 6954]\n",
      "loss: 0.519172  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.542090 \n",
      "\n",
      "loss: 0.600113  [    0/ 6954]\n",
      "loss: 0.498865  [ 3200/ 6954]\n",
      "loss: 0.485070  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.540495 \n",
      "\n",
      "loss: 0.686626  [    0/ 6954]\n",
      "loss: 0.463835  [ 3200/ 6954]\n",
      "loss: 0.510490  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.540520 \n",
      "\n",
      "loss: 0.533485  [    0/ 6954]\n",
      "loss: 0.448208  [ 3200/ 6954]\n",
      "loss: 0.529636  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.542080 \n",
      "\n",
      "loss: 0.662840  [    0/ 6954]\n",
      "loss: 0.619477  [ 3200/ 6954]\n",
      "loss: 0.625192  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.541597 \n",
      "\n",
      "loss: 0.540845  [    0/ 6954]\n",
      "loss: 0.582684  [ 3200/ 6954]\n",
      "loss: 0.691397  [ 6400/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.543776 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    " \n",
    "# 准备数据集\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        xy = data\n",
    "        # xy.shape（）可以得到xy的行列数\n",
    "        self.len = xy.shape[0]\n",
    "        # 选取相关的数据特征\n",
    "        # np.array()将数据转换成矩阵，方便进行接下来的计算\n",
    "        # 要先进行独热表示，然后转化成array，最后再转换成矩阵\n",
    "        self.x_data = torch.from_numpy(np.array(pd.get_dummies(xy.iloc[:,:-1])))\n",
    "        self.y_data = torch.from_numpy(np.array(xy.iloc[:,-1]))\n",
    " \n",
    "    # getitem函数，可以使用索引拿到数据\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    " \n",
    "    # 返回数据的条数/长度\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    " \n",
    " \n",
    "# 实例化自定义类，并传入数据地址\n",
    "data = pd.read_csv('train_full.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(data, test_size=0.2)\n",
    "train_dataset = TitanicDataset(train)\n",
    "valid_dataset = TitanicDataset(valid)\n",
    "# num_workers是否要进行多线程服务，num_worker=2 就是2个进程并行运行\n",
    "# 采用Mini-Batch的训练方法\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    " \n",
    "# 定义模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # 要先对选择的特征进行独热表示计算出维度，而后再选择神经网络开始的维度\n",
    "        # inputsize\n",
    "        self.linear1 = torch.nn.Linear(11, 13)\n",
    "        self.linear2 = torch.nn.Linear(13, 1)\n",
    " \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    " \n",
    "    # 前馈\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    " \n",
    "        return x\n",
    "    \n",
    "    def test(self, x):\n",
    "        with torch.no_grad():\n",
    "            x=self.sigmoid(self.linear1(x))\n",
    "            x=self.sigmoid(self.linear2(x))\n",
    "            y=[]\n",
    "            # 根据二分法原理，划分y的值\n",
    "            for i in x:\n",
    "                if i >0.5:\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "        return y\n",
    " \n",
    "# 实例化模型\n",
    "model = Model()\n",
    " \n",
    "# 定义损失函数\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    " \n",
    "# 防止windows系统报错\n",
    "if __name__ == '__main__':\n",
    "    # 采用Mini-Batch的方法训练要采用多层嵌套循环\n",
    "    # 所有数据都跑100遍\n",
    "    for epoch in range(100):\n",
    "        # data从train_loader中取出数据（取出的是一个元组数据）：（x，y）\n",
    "        # enumerate可以获得当前是第几次迭代，内部迭代每一次跑一个Mini-Batch\n",
    "        size = len(train_loader.dataset)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # inputs获取到data中的x的值，labels获取到data中的y值\n",
    "            x, y = data\n",
    "            x = x.float()\n",
    "            y = y.float()\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_pred.squeeze(-1)\n",
    "            loss = criterion(y_pred, y)\n",
    "            #print(epoch, i, loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                loss, current = loss.item(), i * len(x)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        size = len(valid_loader.dataset)\n",
    "        num_batches = len(valid_loader)\n",
    "        valid_loss, correct = 0, 0\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            # 这块用model.test()代替\n",
    "            with torch.no_grad():\n",
    "                x, y = data\n",
    "                x = x.float()\n",
    "                y = y.float()\n",
    "                pred = model(x)\n",
    "                pred = pred.squeeze(-1)\n",
    "                y_res = []\n",
    "                for p in pred:\n",
    "                    if p > 0.5:\n",
    "                        y_res.append(1)\n",
    "                    else:\n",
    "                        y_res.append(0)\n",
    "                y_res = torch.from_numpy(np.array(y_res).astype(np.float32))\n",
    "                valid_loss += criterion(pred, y).item()\n",
    "                correct += (y_res == y).type(torch.float).sum().item()\n",
    "        valid_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {valid_loss:>8f} \\n\")\n",
    "test_data=pd.read_csv('test.csv')\n",
    "feature = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "test=torch.from_numpy(np.array(pd.get_dummies(test_data[feature])))\n",
    "y=model.test(test.float())\n",
    " \n",
    "# 输出预测结果\n",
    "output=pd.DataFrame({'PassengerId':test_data.PassengerId,'Survived':y})\n",
    "output.to_csv('my_predict.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f76d4fda128b12615e46e0e8dd834a222e7abd956eb53de74309670d1db4104c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
