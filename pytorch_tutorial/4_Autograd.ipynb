{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练神经网络时，最常用的算法是反向传播。在该算法中，根据损失函数相对于给定参数的梯度来调整参数（模型权重）。\n",
    "\n",
    "为了计算这些梯度，PyTorch内置了一个名为torch.autograd的微分引擎。它支持自动计算任何计算图的梯度。\n",
    "\n",
    "考虑最简单的单层神经网络，具有输入x、参数w和b以及一些损失函数。它可以通过以下方式在PyTorch中定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "\"\"\"\n",
    "在这个网络中，w和b是我们需要优化的参数。\n",
    "因此，我们需要能够计算这些变量的损失函数梯度。\n",
    "为此，我们设置了这些张量的requires_grad属性\n",
    "您可以在创建张量时设置requires_grad的值\n",
    "或者稍后使用x.requires_grad_（True）方法设置。\n",
    "\"\"\"\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们应用于张量来构造计算图的函数实际上是function类的对象。\n",
    "\n",
    "该对象知道如何计算正向函数，以及如何在反向传播步骤中计算其导数。\n",
    "\n",
    "反向传播函数的引用存储在张量的grad_fn属性中。您可以在文档中找到函数的更多信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x00000238ED9F0190>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x00000238EFD80C70>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了优化神经网络中参数的权重，我们需要计算损失函数相对于参数的导数。\n",
    "\n",
    "使用loss.backward()计算反向传播的参数\n",
    "\n",
    "然后通过w.grad b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3322, 0.0708, 0.3268],\n",
      "        [0.3322, 0.0708, 0.3268],\n",
      "        [0.3322, 0.0708, 0.3268],\n",
      "        [0.3322, 0.0708, 0.3268],\n",
      "        [0.3322, 0.0708, 0.3268]])\n",
      "tensor([0.3322, 0.0708, 0.3268])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们只能获得计算图的叶节点的grad属性，其中requires_grad属性设置为True。对于图中的所有其他节点，渐变将不可用。\n",
    "出于性能原因，我们只能在给定的图形上使用反向一次来执行梯度计算。如果我们需要对同一个图执行多个反向调用，则需要将retain_graph=True传递给反向调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，requires_grad=True的所有张量都在跟踪其计算历史并支持梯度计算。然而，在某些情况下，我们不需要这样做，例如，当我们训练了模型，只想将其应用于一些输入数据时，即我们只想通过网络进行前向计算。我们可以通过用torch.no_grad()块包围我们的计算代码来停止跟踪计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现相同结果的另一种方法是在张量上使用detach()方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f76d4fda128b12615e46e0e8dd834a222e7abd956eb53de74309670d1db4104c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
