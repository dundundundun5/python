# 通关信息熵

## 前置

1. 正态分布

    $$
    X\sim N(\mu, \sigma^2)\\
    EX=\mu\\
    $$
2. 后续不写了，都是一维随机变量的知识点，数字特征什么的

## 熵
<https://zhuanlan.zhihu.com/p/149186719>

香农信息论：熵是对事件前缀编码后，得到的最小平均编码长度。

3位二进制可以至多表示八种状态。信息熵来源于长度。

其认为，发生概率较低的事件拥有较高的信息量，因为该事件的发生为现实排除了更多的不确定性。


__先考虑一种等可能事件的离散情况__

* 首先要求最小编码长度

1. n种可能性所需要的最小编码长度为$log_2^N$
2. 若所有事件等可能那么$log_2^N=-log_2^{\frac{1}{N}}$
3. $令P=\frac{1}{N}则=-log_2^P$表示所有事件等可能发生时所需的最小编码长度

* 其次要求平均的最小编码长度

1. 只需对频率和长度的乘积求和即可$\sum_{i=1}^NP(-log_2^P)$
2. 代入$P=\frac{1}{N}$得到平均最小编码长度为$-log_2^P$
3. 回到起点，在等可能事件里平均编码长度收敛于最小平均编码长度

__再考虑不等可能事件的离散情况__

1. 平均最小编码长度变为$\sum_{i=1}^NP(i)(-log_2^{P(i)})$
2. $P(i)$表示第$i$个事件发生的可能性
3. 这就是熵在信息论中普遍的定义 

熵大，则说明事件编码长，事件多，分给每个事件的可能性小；熵小，则说明事件编码短，事件少，分给每个事件的可能性多。


## 交叉熵
__仍然考虑不等可能事件的离散情况__
1. 已知有真实数据分布P，理论数据分布Q，熵衡量的是可能性和平均编码长度都来源于P的实际平均最小编码长度，交叉熵衡量的是可能性来源于为P、编码长度来源于Q的情况下，特殊平均最小编码长度。
2. 交叉熵写作：$\sum_{i=1}^NP(i)(-log_2^{Q(i)})$
3. 交叉熵表示，当使用理论的Q去拟合实际的P时所需的理论平均最小编码长度。
4. 可能性和平均编码长度都来源于Q的理论平均最小编码长度仅仅和Q有关，纯理论脱离现实。

## 严谨

上述例子为事件为有限N个的离散情况，现作出前置条件声明
1. 事件个数扩展到无穷个
2. 求和符号用数学期望表示
3. 工程上计算log底数一般为e，以下不做修改

得到熵$E_{x\sim P(x)}[-log_2^{P(x)}]$和交叉熵$E_{x\sim P(x)}[-log_2^{Q(x)}]$

离散和连续情况下，时间步长i分别为整数和实数 

* 离散： $\sum_{i=-\infty}^{+\infty}[-P(i)log_2^{P(i)}]$和$\sum_{i=-\infty}^{+\infty}[-P(i)log_2^{Q(i)}]$
* 连续： $\int_{-\infty}^{+\infty}[-P(i)log_2^{P(i)}]di$和$\int_{-\infty}^{+\infty}[-P(i)log_2^{Q(i)}]di$

交叉熵是对称的，即P和Q的交叉熵等于Q和P的交叉熵，因此可用作距离度量
## 相对熵
kullback–leibler divergence
相对熵是交叉熵的进阶表示，又称为KL散度，后面全用相对熵的说法。相对熵衡量了两个分布之间的差异（非距离），仍然P为实际数据分布，Q为理论数据分布，Entropy为熵，CrossEntropy为交叉熵，RelativeEntropy为相对熵，则用Q拟合P的相对熵定义为

$$\begin{align}
    RelativeEntropy(P,Q)&=CrossEntropy(P,Q) -Entropy(P)\notag\\
    &=E_{x\sim P(x)}[-log_2^{Q(x)}]-E_{x\sim P(x)}[-log_2^{p(x)}]\notag\\
    &=E_{x\sim P(x)}[log_2^{\frac{P(x)}{Q(x)}}]\notag\\
    &=\sum_{i=-\infty}^{+\infty}[P(i)log_2^{\frac{P(i)}{Q(i)}}](离散)\notag\\
    &=\int_{-\infty}^{+\infty}[P(i)log_2^{\frac{P(i)}{Q(i)}}]di（连续）
\end{align}$$ 

$$\begin{align}
    RelativeEntropy(Q,P)&=CrossEntropy(Q,P) -Entropy(Q)\notag\\
    &=\sum_{i=-\infty}^{+\infty}[Q(i)log_2^{\frac{Q(i)}{P(i)}}](离散)\notag\\
    &=\int_{-\infty}^{+\infty}[Q(i)log_2^{\frac{Q(i)}{P(i)}}]di（连续）  
\end{align}$$ 



## 关于相对熵

由于相对熵作为函数时并不对称，即$RelativeEntropy(P,Q)\neq RelativeEntropy(Q,P)$，相对熵无法作为距离度量函数，所以相对熵只能说

在用作损失函数时，由于交叉熵是不固定的，熵是固定的，工程上便取巧只使用交叉熵作为损失函数


## VAE中相对熵的推导
现始终考虑连续情况
1. 假设P(z)是标准正态分布
1. $P(z|x)=\frac{P(x|z)P(z)}{P(x)}=\int_{-\infty}^{+\infty} \frac{P(x|z)P(z)}{P(x)}dz$是贝叶斯公式
2. 变分推断就是已知$P(z,x)=P(x|z)P(z)$，求后验分布$P(z|x)$，方法是假设一个变分分布$Q(z)$，通过最小化相对熵来让$Q(z)$拟合$P(z|x)$

$$
\begin{align}
    \min RelativeEntropy(Q(z),P(z|x))&=\min E_{Q(z)}[log_2^{\frac{Q(z)}{P(z|x)}}]\notag\\
    &=\min E_{Q(z)}[log_2^{\frac{Q(z)P(x)}{P(x|z)P(z)}}]\notag\\
    &=\min E_{Q(z)}[log_2^{\frac{Q(z)}{P(x|z)P(z)}}]+log_2^{P(x)} 去掉最后常数项\notag\\
    &=\min E_{Q(z)}[log_2^{\frac{Q(z)}{P(z)}}-log_2^{P(x|z)}]\notag\\
    &=\min RelativeEntropy(Q(z),P(z)) + E_Q(z)[-log_2^{P(x|z)}]\notag
\end{align}
$$
左侧是解码部分的相对熵，右侧是z生成x的重构损失,左侧小代表能成功用P(z)拟合变分分布Q(z),右侧小代表z生成x的概率很大，说明重组的还原能力强。

最后推一下编码部分正态分布p1和标准正态分布p2的相对熵
$$
\begin{align}
     RelativeEntropy(P_1,P_2)&=\int P_1(x)\log\frac{P_1(x)}{P_2(x)}dx\notag\\
     &=\int P_1(x)\log\frac{\frac{1}{\sqrt{2\pi\sigma_1^2}}e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}}}{\frac{1}{\sqrt{2\pi\sigma_2^2}}e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}}}dx\quad log替换为ln\notag\\
     &=\int P_1(x)[\ln\frac{\sigma_2}{\sigma_1} - \frac{(x-\mu_1)^2}{2\sigma_1^2}+\frac{(x-\mu_2)^2}{2\sigma_2^2}]dx\notag\\
     &=ln\frac{\sigma_2}{\sigma_1}-\frac{1}{2} + \sigma_1^2+\mu_1^2\notag
\end{align}
$$
1. 第一项与x无关，可提到积分外，而正态分布概率密度在全域上的积分为1
2. 第二项可以把负号和分母提到积分号外$-\frac{1}{2\sigma_1^2}\int P_1(x)(x-\mu_1)^2dx$，实际上计算的是$\int P_1(x)(x-\mu_1)^2dx$，而$\int P_1(x)(x-\mu_1)^2dx=E(x-\mu_1)^2$，由方差公式可得$D(x-\mu_1)=E(x-\mu_1)^2-[E(x-\mu_1)]^2=\sigma_1^2$，所以最后变成$-\frac{1}{2}$
3. 第三项就比较困难了，现在可以把$P2\sim N(0,1)$代入则得到$\frac{1}{2\sigma_2^2}\int P_1(x)x^2dx=\frac{1}{2\sigma_2^2}[EX^2]=\frac{1}{2\sigma_2^2}[DX+(EX)^2]=\sigma_1^2+\mu_1^2$

## GAN中相对熵的影子

<http://speech.ee.ntu.edu.tw/~tlkagk/slide/Tutorial_HYLee_GAN.pdf>

<https://zhuanlan.zhihu.com/p/266677860>

相对熵又名KL散度，然而GAN没有用到KL散度，二是用到了JS散度，其为KL散度的变形，下面说到的概率分布就是概率密度
$$
\begin{align}
    \min_G\max_D V(G,D)=\min_G\max_D E_{P(x)}[\log D(x)] +E_{P(z)}\log[1-D\big(G(z)\big)]\notag
\end{align}
$$

其中，D是分辨器，G是生成器，分辨器用于区分样本是否为真实样本，生成器用于从随机数据中采样并编码为虚假样本，GAN的目的是让分辨器无法区分真实和虚假样本，

* 极大似然估计思想

1. 有实际分布$P(x)$，现取理论分布$Q(x;\theta)$
2. 求$\theta$处于何值时，$Q(x;\theta)\rightarrow P(x)$

* 极大似然估计

1. $Q(x;\theta)$表示的是x出现的概率分布，那么来源于$P(x)$的x代入$Q(x;\theta)$，其每一项的概率都应该尽可能大，因此极大似然估计要求$\max\prod_{i=-\infty}^{+\infty}Q(x_i;\theta)$
2. 实际上由于x是连续变量，很难通过实际数据推出实际数据分布的通式，因此往往采样m个点
3. 极大似然估计是一种拟合的方法

* GAN

1. 固定G，优化D，对于D，$P(x)和P(z)$又是已知的
$$
\begin{align}
    V(G固定,D)&=E_{P(x)}[\log D(x)] +E_{P(z)}\log[1-D\big(G(z)\big)]\notag\\
    &=E_{P(x)}[\log D(x)] +E_{P(z)}\log[1-D(x)]\notag\\
    &=\int P(x)\log D(x)dx + \int P(z)\log\big(1- D(x)\big)dx\notag\\
    f(D)&=a\log D + b\log(1- D)\notag\\
    a &= P(x)\quad b=P(z)\notag
\end{align}
$$

2. 求$f(D)$驻点$D*$即可

$$
\begin{align}
    f'(D^*)&=a\frac{log^e}{D^*}  + b\frac{log^e}{1-D^*}\times (-1)=0\notag\\
    &=\frac{a}{D^*}-\frac{b}{1-D^*}=0\notag\\
    &a=(b+a)D^*\notag\\
    D^*&=\frac{a}{a+b}\notag
\end{align}
$$

3. 代入

$$
\begin{align}
    V(G固定,D^*)&=\int P(x)\log \frac{P(x)}{P(x)+P(z)}dx + \int P(z)\log\big(1- \frac{P(x)}{P(x)+P(z)}\big)dx\notag\\
    &=\int P(x)[\log \frac{P(x)}{P(x)+P(z)}+\log2-log2]dx + \int P(z)[\log\big(\frac{P(z)}{P(x)+P(z)}+log2-log2]\big)dx\notag\\
    &=\int P(x)[\log \frac{P(x)}{\frac{P(x)+P(z)}{2}}]dx + \int P(z)[\log\frac{P(z)}{\frac{P(x)+P(z)}{2}}]dx-\int \big(P(x)+P(z)\big)\log 2dx\notag\\
    &=RelativeEntropy\big(P(x),\frac{P(x)+P(z)}{2}\big)+RelativeEntropy\big(P(z),\frac{P(x)+P(z)}{2}\big)-2\log2\notag\\
    &=2JS(P(x),P(z))-2\log2
\end{align}
$$  

这里有个小技巧，两个积分同时加一个log2再减去log2，保证两个密度叠加后概率总和仍然为1

4. JS散度

JS散度是相对熵的变体，JS散度是对称的
$$
\begin{align}
    JS(P,Q) = \frac{1}{2}RelativeEntropy(P,\frac{P}{P+Q})+\frac{1}{2}RelativeEntropy(P,\frac{Q}{P+Q})\notag
\end{align}
$$

因此$\max_D V(G,D)$衡量了虚假样本和真实样本的分布

$$
\begin{align}
    \max_D V(G,D)=\max_D E_{P(x)}[\log D(x)] +E_{P(z)}\log[1-D\big(G(z)\big)]\notag
\end{align}
$$

5. 固定D，优化G


    由于G优化的表达式，是由固定G时求得的（此处G的优化可能指代G的参数在邻域内变化；如果G发生大幅度改变，那么优化将不再能套用JS散度）

    只需优化$P(x)和P(z)$的JS散度即可求得优化的$G^*$
---