<font size=6>

# 通关信息熵

## 前置

1. 正态分布

    $$
    X\sim N(\mu, \sigma^2)\\
    EX=\mu\\
    $$
2. 一维随机变量的知识点，数字特征的计算公式

## 熵
<https://zhuanlan.zhihu.com/p/149186719>

香农信息论：熵是对事件前缀编码后，得到的最小平均编码长度。

3位二进制可以至多表示八种状态。信息熵来源于长度。

其认为，发生概率较低的事件拥有较高的信息量，因为该事件的发生为现实排除了更多的不确定性。


__先考虑一种等可能事件的离散情况__

* 首先要求最小编码长度

1. n种可能性所需要的最小编码长度为$log_2^N$
2. 若所有事件等可能那么$log_2^N=-log_2^{\frac{1}{N}}$
3. $令P=\frac{1}{N}则=-log_2^P$表示所有事件等可能发生时所需的最小编码长度

* 其次要求平均的最小编码长度

1. 只需对频率和长度的乘积求和即可$\sum_{i=1}^NP(-log_2^P)$
2. 代入$P=\frac{1}{N}$得到平均最小编码长度为$-log_2^P$
3. 回到起点，在等可能事件里平均编码长度收敛于最小平均编码长度

__再考虑不等可能事件的离散情况__

1. 平均最小编码长度变为$\sum_{i=1}^NP(i)(-log_2^{P(i)})$
2. $P(i)$表示第$i$个事件发生的可能性
3. 这就是熵在信息论中普遍的定义 

总结，熵大，则说明事件编码长，事件多，事件的平均可能性小；熵小，则说明事件编码短，事件少，事件的平均可能性大。


## 交叉熵
__仍然考虑不等可能事件的离散情况__
1. 已知有真实数据分布P，理论数据分布Q，熵衡量的是可能性和平均编码长度都来源于P的实际平均最小编码长度，交叉熵衡量的是可能性来源于为P、编码长度来源于Q的情况下，特殊平均最小编码长度。
2. 交叉熵写作：$\sum_{i=1}^NP(i)(-log_2^{Q(i)})$
3. 交叉熵表示，当使用理论的Q去拟合实际的P时所需的理论平均最小编码长度。
4. 可能性和平均编码长度都来源于Q的理论平均最小编码长度仅仅和Q有关，纯理论脱离现实。

## 严谨

上述例子为事件为有限N个的离散情况，现作出前置条件声明
1. 事件个数扩展到无穷个
2. 求和符号用数学期望表示
3. 工程上计算log底数一般为e，以下不做修改

得到熵$E_{x\sim P(x)}[-log_2^{P(x)}]$和交叉熵$E_{x\sim P(x)}[-log_2^{Q(x)}]$

离散和连续情况下，时间步长i分别为整数和实数 

* 离散： $\sum_{i=-\infty}^{+\infty}[-P(i)log_2^{P(i)}]$和$\sum_{i=-\infty}^{+\infty}[-P(i)log_2^{Q(i)}]$
* 连续： $\int_{-\infty}^{+\infty}[-P(i)log_2^{P(i)}]di$和$\int_{-\infty}^{+\infty}[-P(i)log_2^{Q(i)}]di$

交叉熵是对称的，即P和Q的交叉熵等于Q和P的交叉熵，因此可用作距离度量
## 相对熵
kullback–leibler divergence
相对熵是交叉熵的进阶表示，又称为KL散度，后面全用相对熵的说法。相对熵衡量了两个分布之间的差异（非距离），仍然P为实际数据分布，Q为理论数据分布，Entropy为熵，CrossEntropy为交叉熵，RelativeEntropy为相对熵，则用Q拟合P的相对熵定义为

<font color="red">___后文的小p小q与这里的PQ无关，只需记住一点，相对熵的前者分布为实际分布(后验分布)，后者为理论分布(先验分布)___</font>
$$\begin{align}
    RelativeEntropy(P,Q)&=CrossEntropy(P,Q) -Entropy(P)\notag\\
    &=E_{x\sim P(x)}[-log_2^{Q(x)}]-E_{x\sim P(x)}[-log_2^{p(x)}]\notag\\
    &=E_{x\sim P(x)}[log_2^{\frac{P(x)}{Q(x)}}]\notag\\
    &=\sum_{i=-\infty}^{+\infty}[P(i)log_2^{\frac{P(i)}{Q(i)}}](离散)\notag\\
    &=\int_{-\infty}^{+\infty}[P(i)log_2^{\frac{P(i)}{Q(i)}}]di（连续）
\end{align}$$ 

$$\begin{align}
    RelativeEntropy(Q,P)&=CrossEntropy(Q,P) -Entropy(Q)\notag\\
    &=\sum_{i=-\infty}^{+\infty}[Q(i)log_2^{\frac{Q(i)}{P(i)}}](离散)\notag\\
    &=\int_{-\infty}^{+\infty}[Q(i)log_2^{\frac{Q(i)}{P(i)}}]di（连续）  
\end{align}$$ 



## 关于相对熵

由于相对熵作为函数时并不对称，即$RelativeEntropy(P,Q)\neq RelativeEntropy(Q,P)$，相对熵无法作为距离度量函数，所以相对熵只能说

__在用作损失函数时，由于要通过损失函数求得负梯度，因此时常提出一个负号，log里面提负号很简单__
## 似然估计
* 极大似然估计思想

1. 有实际分布$P(x)$，现取理论分布$Q(x;\theta)$
2. 求$\theta$处于何值时，$Q(x;\theta)\rightarrow P(x)$

* 极大似然估计

1. $Q(x;\theta)$表示的是x出现的概率分布，那么来源于$P(x)$的x代入$Q(x;\theta)$，其每一项的概率都应该尽可能大，因此极大似然估计要求$\max\prod_{i=-\infty}^{+\infty}Q(x_i;\theta)$
2. 实际上由于x是连续变量，很难通过实际数据推出实际数据分布的通式，因此往往采样m个点
3. 极大似然估计是一种拟合的方法
## 重参数
<https://gregorygundersen.com/blog/2018/04/29/reparameterization/>

这篇blog很好，本段只是对原文的精炼，属于转载


1. 考虑
$$
\begin{align}
    E_{p(z)}[f_{\theta}(z)]\notag
\end{align}
$$
此时z是隐藏变量，符合固定概率密度，求$f_{\theta}(z)$的期望，这里类似于极大似然估计，对于函数f(z)，我们只知道函数值f(z)的分布不知道f(z)的表达式，却要求f(z)的期望。假设有一个$\theta$能使$f_{\theta}(z)$最大或最小，那么就要找出最优化的这个$\theta$
$$
\begin{align}
    \nabla_{\theta} E_{p(z)}[f_{\theta}(z)]&=\nabla_{\theta} [\int_z p(z)f_{\theta}(z)dz]\notag\\
    &=\int_z p(z)[\nabla_{\theta} f_{\theta}(z)]dz\notag\\
    &=E_{p(z)}[\nabla_{\theta} f_{\theta}(z)]\notag
\end{align}
$$
这边梯度是可以放入积分符号的，因为是对$\theta$求梯度，对z求积分。然而，不能笼统认为分布固定的情况下用极大似然估计求最优函数值，隐藏变量的分布也是跟$\theta$变化的，即
$$
\begin{align}
    E_{p_{\theta}(z)}[f_{\theta}(z)]\notag\\
    \nabla_{\theta} E_{p_{\theta}(z)}[f_{\theta}(z)]&=\nabla_{\theta} [\int_z p_{\theta}(z)f_{\theta}(z)dz]\notag\\
    &=\int_z p_{\theta}(z)[\nabla_{\theta} f_{\theta}(z)]dz+\int_z [\nabla_{\theta}p_{\theta}(z)] f_{\theta}(z)dz\notag\\
    &= \int_z f_{\theta}(z)[\nabla_{\theta} f_{\theta}(z)]dz+E_{p_{\theta}(z)}[\nabla_{\theta} f_{\theta}(z)]\notag
\end{align}
$$

第三行是变量乘法的导数公式$(uv)'=u'v+uv'$。最后一步是分2个式子，前者是不能变化到期望公式的，因为$f_{\theta}(z)$是目标函数不一定是概率密度
<font color ="red"> __目前的方法无法用于求解，故引出重参数技巧__</font>
$$
\begin{align}
    E_{p_{\theta}(z)}[f_{\theta}(z)]\notag\\
    \epsilon\sim p(\epsilon)\notag\\
    z=g_{\theta}(x,\epsilon)\notag\\
    \nabla_{\theta} E_{p_{\theta}(z)}[f_{\theta}(z)]&=\nabla_{\theta} E_{p(\epsilon)}[f(g_{\theta}(x,\epsilon))]\notag\\
    &= E_{p(\epsilon)}[\nabla_{\theta}f(g_{\theta}(x,\epsilon))]\notag\\
    &= \frac{1}{m}\sum_{i=1}^m\nabla_{\theta}f(g_{\theta}(x,\epsilon^{(i)}))\notag
\end{align}
$$

通过定义固定分布的$\epsilon$来转移无法对带参数$\theta$的z求梯度的问题，<font color ="red"> __因此问题没解决，只是转移了__</font>，最后一步使用Monte Carlo方法得到近似解。这是一篇2018的博客，其实从现在的眼光看，g是生成器的前身，满足固定分布的$\epsilon$很像GAN里的噪声，以及VAE隐藏变量里满足正态分布变量的假设

__此处解释了为什么不直接从正态分布$p_{\theta}(z)$采样！采样可行，但是求梯度不一定可行！__
## VAE中的相对熵


### 怎么回事

1. VAE是为了求极大似然估计$\log p_{\theta}(x|z)$，最终要求的是ELBO Evidence Lower BOund (ELBO).下界越小似然估计就越大，这是VAE损失函数的由来
2. 现有数据x，和模型p以及模型得到的理论分布$p_{\theta}(x,z)$
3. 需要推断现实数据$q_{\phi}(z|x)$
4. 构造变分分布$p_{\theta}(z)$，通过p去拟合q达到推断目的
5. 我可以假设$p_{\theta}(z)$是正态分布，但是由于上述原因（重参数章节），求梯度真的不方便，因此通过转移的方式（变分推断思想体现）

### 相对熵（现实vs理论）
$$
\begin{align}
    RelativeEntropy(q_{\phi}(z|x),p_{\theta}(x,z))&=E_{q_{\phi}(z|x)}[log_2^{\frac{q_{\phi}(z|x)}{p_{\theta}(x,z)}}]\notag\\
    &=E_{q_{\phi}(z|x)}[log_2^{\frac{q_{\phi}(z|x)p_{\theta}(x)}{p_{\theta}(x|z)p_{\theta}(z)}}]\notag\\
    &=E_{q_{\phi}(z|x)}[log_2^{\frac{q_{\phi}(z|x)}{p_{\theta}(x|z)p_{\theta}(z)}}]+log_2^{p_{\theta}(x)} 去掉最后常数项\notag\\
    &=E_{q_{\phi}(z|x)}[log_2^{\frac{q_{\phi}(z|x)}{p_{\theta}(z)}}-log_2^{p_{\theta}(x|z)}]\notag\\
    &=RelativeEntropy(q_{\phi}(z|x),p_{\theta}(z)) + E_{q_{\phi}(z|x)}[-log_2^{p_{\theta}(x|z)}]\notag
\end{align}
$$

### VAE损失函数的相对熵

$$
\begin{align}
    ELBO(\theta,\phi)&=E_{q_{\phi}(z|x)}[\log p_{\theta}(x,z)-\log q_{\phi}(z|x)]\notag\\
    &=-E_{q_{\phi}(z|x)}[\log q_{\phi}(z|x)-\log p_{\theta}(x,z)]\notag\\
    &=-RelativeEntropy(q_{\phi}(z|x),p_{\theta}(x,z))\notag\\
    &=-RelativeEntropy(q_{\phi}(z|x),p_{\theta}(z)) + E_{q_{\phi}(z|x)}[log^{p_{\theta}(x|z)}]\notag\\
    &=重参数+蒙特卡洛近似解\quad z=g_{\phi}(x,\epsilon)\quad \epsilon\sim p(\epsilon)\notag\\
    &假设z\sim N(\mu,\sigma^2)\quad \epsilon\sim N(0,1)\quad z=\mu+\sigma\epsilon\notag\\
    &=-RelativeEntropy(N(\mu,\sigma^2),N(0,1)) + \frac{1}{m}\sum_{i=1}^m\log^{p_{\theta}(x|z^{(i)})}\notag\\

  
\end{align}

$$
__其实原文没有对重参数代入之后分布变换的严谨解释，附录里也没有，我自己推了很久还是觉得不严谨，所以此处不作介绍__

最后推一下编码部分正态分布p1和标准正态分布p2的相对熵
$$
\begin{align}
     RelativeEntropy(P_1\sim N(\mu,\sigma^2),P_2\sim N(0,1))&=\int P_1(x)\ln
     \frac{P_1(x)}{P_2(x)}dx\notag\\
     &=\int P_1(x)\ln\frac{\frac{1}{\sqrt{2\pi\sigma_1^2}}e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}}}{\frac{1}{\sqrt{2\pi\sigma_2^2}}e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}}}dx\quad log替换为ln\notag\\
     &=\int P_1(x)[\ln\frac{\sigma_2}{\sigma_1} - \frac{(x-\mu_1)^2}{2\sigma_1^2}+\frac{(x-\mu_2)^2}{2\sigma_2^2}]dx\notag\\
     &=ln\frac{\sigma_2}{\sigma_1}-\frac{1}{2} + \sigma_1^2+\mu_1^2\notag
\end{align}
$$
1. 第一项与x无关，可提到积分外，而正态分布概率密度在全域上的积分为1
2. 第二项可以把负号和分母提到积分号外$-\frac{1}{2\sigma_1^2}\int P_1(x)(x-\mu_1)^2dx$，实际上计算的是$\int P_1(x)(x-\mu_1)^2dx$，而$\int P_1(x)(x-\mu_1)^2dx=E(x-\mu_1)^2$，由方差公式可得$D(x-\mu_1)=E(x-\mu_1)^2-[E(x-\mu_1)]^2=\sigma_1^2$，所以最后变成$-\frac{1}{2}$
3. 第三项就比较困难了，现在可以把$P2\sim N(0,1)$代入则得到$\frac{1}{2\sigma_2^2}\int P_1(x)x^2dx=\frac{1}{2\sigma_2^2}[EX^2]=\frac{1}{2\sigma_2^2}[DX+(EX)^2]=\sigma_1^2+\mu_1^2$

## GAN中相对熵的影子

<http://speech.ee.ntu.edu.tw/~tlkagk/slide/Tutorial_HYLee_GAN.pdf>

<https://zhuanlan.zhihu.com/p/266677860>

相对熵又名KL散度，然而GAN没有用到KL散度，二是用到了JS散度，其为KL散度的变形，下面说到的概率分布就是概率密度
$$
\begin{align}
    \min_G\max_D V(G,D)=\min_G\max_D E_{P(x)}[\log D(x)] +E_{P(z)}\log[1-D\big(G(z)\big)]\notag
\end{align}
$$

其中，D是分辨器，G是生成器，分辨器用于区分样本是否为真实样本，生成器用于从随机数据中采样并编码为虚假样本，GAN的目的是让分辨器无法区分真实和虚假样本，



* GAN

1. 固定G，优化D，对于D，$P(x)和P(z)$又是已知的
$$
\begin{align}
    V(G固定,D)&=E_{P(x)}[\log D(x)] +E_{P(z)}\log[1-D\big(G(z)\big)]\notag\\
    &=E_{P(x)}[\log D(x)] +E_{P(z)}\log[1-D(x)]\notag\\
    &=\int P(x)\log D(x)dx + \int P(z)\log\big(1- D(x)\big)dx\notag\\
    f(D)&=a\log D + b\log(1- D)\notag\\
    a &= P(x)\quad b=P(z)\notag
\end{align}
$$

2. 求$f(D)$驻点$D*$即可

$$
\begin{align}
    f'(D^*)&=a\frac{log^e}{D^*}  + b\frac{log^e}{1-D^*}\times (-1)=0\notag\\
    &=\frac{a}{D^*}-\frac{b}{1-D^*}=0\notag\\
    &a=(b+a)D^*\notag\\
    D^*&=\frac{a}{a+b}\notag
\end{align}
$$

3. 代入

$$
\begin{align}
    V(G固定,D^*)&=\int P(x)\log \frac{P(x)}{P(x)+P(z)}dx + \int P(z)\log\big(1- \frac{P(x)}{P(x)+P(z)}\big)dx\notag\\
    &=\int P(x)[\log \frac{P(x)}{P(x)+P(z)}+\log2-log2]dx + \int P(z)[\log\big(\frac{P(z)}{P(x)+P(z)}+log2-log2]\big)dx\notag\\
    &=\int P(x)[\log \frac{P(x)}{\frac{P(x)+P(z)}{2}}]dx + \int P(z)[\log\frac{P(z)}{\frac{P(x)+P(z)}{2}}]dx-\int \big(P(x)+P(z)\big)\log 2dx\notag\\
    &=RelativeEntropy\big(P(x),\frac{P(x)+P(z)}{2}\big)+RelativeEntropy\big(P(z),\frac{P(x)+P(z)}{2}\big)-2\log2\notag\\
    &=2JS(P(x),P(z))-2\log2
\end{align}
$$  

这里有个小技巧，两个积分同时加一个log2再减去log2，保证两个密度叠加后概率总和仍然为1

4. JS散度

JS散度是相对熵的变体，JS散度是对称的
$$
\begin{align}
    JS(P,Q) = \frac{1}{2}RelativeEntropy(P,\frac{P}{P+Q})+\frac{1}{2}RelativeEntropy(P,\frac{Q}{P+Q})\notag
\end{align}
$$

因此$\max_D V(G,D)$衡量了虚假样本和真实样本的分布

$$
\begin{align}
    \max_D V(G,D)=\max_D E_{P(x)}[\log D(x)] +E_{P(z)}\log[1-D\big(G(z)\big)]\notag
\end{align}
$$

5. 固定D，优化G


    由于G优化的表达式，是由固定G时求得的（此处G的优化可能指代G的参数在邻域内变化；如果G发生大幅度改变，那么优化将不再能套用JS散度）

    只需优化$P(x)和P(z)$的JS散度即可求得优化的$G^*$
---